{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Yuniverse","text":""},{"location":"#hi-this-is-yun","title":"\ud83d\udc4b Hi, this is Yun","text":"Python<pre><code>class Me:\ndef __init__(self, name, career, position, projects, skills, speak):\nself.name = Yun\nself.career = 'Zilliz (https://zilliz.com/)'\nself.position = 'Technical Writer'\nself.projects = ['Milvus', 'KubeSphere']\nself.skills = ['Technical Writing', 'Editing', 'Proofreading', 'Git', 'Linux', 'Kubernetes']\nself.learn = ['Python','Machine Learning']\nself.speak = ['English', 'Chinese']\ndef introduce(self):\nprint(\"Hi there, this is \" + self.name + \".\")\nprint(\"I'm currently working as a \" + self.position + \"at\" + self.career + \".\")\nprint(\"I'm an advocate for open source and have contributed to projects such as \" + self.projects + \".\")\nprint(\"My skills include \" + \", \".join(self.skills) + \".\")\nprint(\"Currently, I'm learning \" + \", \".join(self.learn) + \".\")\nprint(\"As a language student at university, I'm fluent in \" + \", \".join(self.speak) + \".\")\n# Create an instance of the Me class\nme = Me()\n# Call the introduce() method to introduce yourself\nme.introduce()\n</code></pre>"},{"location":"#care-to-contact","title":"\ud83d\udc50 Care to contact?","text":""},{"location":"about/","title":"About Me","text":"<p>I learn and write things.</p>"},{"location":"concepts/","title":"Concepts","text":"<code>String</code> <p>A string is a collection of characters, mostly treated as a single unit. A string is used to store text values like a name, address, message, etc.</p> <code>Slice</code> <p>A slice is a subset of characters from a string. We use the syntax <code>[ i : f ]</code> to specify the initial and final index of the string to access a slice. There are several ways to access a slice of a string:</p> Text Only<pre><code>- Use both values, as in [ i : f ], to access the slice from index i to index f-1.\n- Use only the intial value, as in [ i : ], to access the slice from index i to the end of the string.\n- Use only the final value, as in [ : f ], to access the slice from the start of the string to index f-1.\n</code></pre> <code>Nested list</code> <p>A nested list is a list that contains other lists as its values or members. The container list is termed an outer list, and the member list is termed an inner list. Example: <code>alist = ['aa', ['b', 'c'], 'dd']</code>.</p>"},{"location":"docs-as-code/","title":"Why Docs as Code Should be Part of Your Dev Cycle","text":"<p>Many established methods help streamline software development. However, the same cannot be said about documentation. Typically, a documentation cycle, as opposed to a developer one, is unnecessarily complicated, being a hybrid of different practices.</p> <p>It's a process that most often only a documentation team knows how to navigate, with developers trying to avoid being part of it at all costs. Nobody wants to learn new tools and processes for a collaboration that does not seem to bring much value.</p> <p>This article attempts to bridge the gap between documentation and development by showing the value of well-plannd documentation and outlining how developers and technical content creators can collaborate under one process that is efficient for both teams.</p> <p>Because, let's face it, software documentation is important. It is often the very face of the product, like when it comes to an API reference. This is why documentation could and should be part of the development cycle, and Docs as Code is the way forward.</p>"},{"location":"docs-as-code/#what-is-a-docs-as-code-approach","title":"What is a Docs as Code approach","text":"<p>As the name suggests, Docs as Code treats documentation as code, using the same tools and processes as software development to manage and publish documentation.</p> <p>Before we delve deeper, let's go through key aspects of a Docs as Code approach to get familiar with the concept.</p>"},{"location":"docs-as-code/#key-aspects","title":"Key aspects","text":"<p>The adoption of Docs as Code represents a paradigm shift in the way technical documentation is created and managed, making it an asset rather than a burden - a mindset many software companies develop around content.</p> <p>It empowers developers to easily update and iterate on documentation, ensures its accuracy and relevant, and provides users with quality self-serve access to information when they need it most.</p> <p>Many wins come with Docs as Code, but let's list those crucial for a developer cycle, the content iteself and collaboration between developers and content creators.</p>"},{"location":"docs-as-code/#written-in-plain-text-format","title":"Written in plain-text format","text":"<p>The most important aspect of the Docs as Code philosophy is efficient collaboration between teams. To cater to this need, documentation is typically stored in a plain-text format, most likely markdown.</p> <p>This facilitates accessing, managing, and collaborating on any document. Plain-text format ensures you need no special equipment, software, or license to work on a document. Anybody with access to the repo where the document is stored can go in and contribute.</p> <p>Styling is also abstracted from both writing and review process. Syntax review is typically automated by a linter and grammer checker, such as a markdown linting expansion plugged into your Visual Studio Code editor. There is no need for a human reviewer to spend time reviewing syntax. Their sole concern is the content itself and the structure of the document.</p>"},{"location":"docs-as-code/#decoupled-frontend-and-backend","title":"Decoupled frontend and backend","text":"<p>The decoupled frontend and backend architecture is a key element of the Docs as Code practices as it separates the concerns of content creation and presentation.</p> <p>This approach frees technical content creators from thinking about page design and styling of content elements (think warning notes, tips, tables, bullet points, etc.). They can focus solely on creating informative and well-structured content while collaborating with frontend developers or designers to provide an engaging and user-friendly presentation layer.</p> <p></p> <p>Content developers are free to deal with the most important aspects of content creation and management.</p>"},{"location":"docs-as-code/#content-structure","title":"Content structure","text":"<p>This is how to present information on a page.</p> <ul> <li> <p>Sections needed for different types of content, such as a tutorial, will always start with a description of what a user will end up with once they complete all the steps.</p> </li> <li> <p>The structure of sections, for instance, a Prerequisites section, can be just in a bullet list, while an introduction section needs at least one paragraph to be considered valid for its purpose.</p> </li> <li> <p>The style of section titles. Is it camel case or like a sentence? Do we want to use gerunds or infinitives - \"Sending an SMS with Python\" or \"Send an SMS with Python\"?</p> </li> </ul>"},{"location":"docs-as-code/#content-findability","title":"Content findability","text":"<p>This is how to present information within the context of an entire documentation site.</p> <ul> <li> <p>Is the content easy to find by foraging through a documentation site, or is it impossible to find manually?</p> </li> <li> <p>How well does the search function work, and how do you optimize it for content findability? (Do we need better indexing, tags, filtering, etc.)</p> </li> </ul>"},{"location":"docs-as-code/#content-resuability","title":"Content resuability","text":"<p>This deals with which content can be reused across multiple guides and how we can best implement it in practice. It is especially important for companies that generate a lot of content that needs to be maintained and kept up to date with as little effort as possible.</p>"},{"location":"docs-as-code/#information-architecture","title":"Information architecture","text":"<p>This is how to organize content into content buckets (for instance, which articles we need in a Getting Started bucket).</p>"},{"location":"docs-as-code/#user-journey","title":"User journey","text":"<p>This is how to navigate through content and link it in a logical way for a quality user experience.</p> <p>As you can see, by adopting a decoupled frontend and backend approach, the technical content team can maximize their productivity and deliver high-quality content.</p>"},{"location":"docs-as-code/#store-content-in-a-version-control-system","title":"Store content in a version control system","text":"<p>The Docs as Code approach embraces the practice of storing documentation in a version control system, leveraging a system the development team already uses.</p> <p>It is important to highlight that the version control system needs to be git-based. A Docs as Code approach assumes that documentation should follow a similar workflow and versioning system as the codebase. By using a git-based product or a similar VCS, technical writers can branch, merge and track changes to the documentation just like developers do with their code.</p> <p>This consistency in version control practices ensures that documentation evolves alongside the software, keeping it up to date and relevant.</p>"},{"location":"docs-as-code/#use-automated-testing-and-development-tools","title":"Use automated testing and development tools","text":"<p>The Docs as Code approach uses automated testing and deployment tools to streamline a documentation development and publication process.</p> <p>It adopts a standard deployment process and uses something easily manageable to publish the docs from the source files, like a static site generator. That, together with automation tools running checks without the need for a human to do it manually, allows a documentation team to adhere to the principles of CI/CD, just like their developer colleagues.</p> <p></p> <p>It is a process that promotes efficiency and scalability, allowing teams to manage larger documentation projects without major hiccups.</p>"},{"location":"docs-as-code/#go-through-a-peer-review","title":"Go through a peer review","text":"<p>Having docs peer review sitting at the same level as code peer review, we make sure documentation is not treated any differently from code, making it harder to omit in the entire developer cycle.</p> <p>One of the key advantages of a Docs as Code peer review is its seamless ability to incorporate feedback from multiple reviewers. Typically, a standard documentation review involves another technical author performing an editorial review and a review from a subject-matter expert, often a developer who worked on the feature.</p> <p>This is fine for an initial handover, but it doesn\u2019t provide an objective story. It\u2019s like asking a parent to describe their child objectively. They can\u2019t, because they created it, shaped it, put a lot of time and effort into its development and are very proud of it.</p> <p>This is why inviting other stakeholders to the proverbial review table is important so they can pitch in their unique perspectives.</p> <p>And just like that, a single-minded document suddenly becomes an exhaustive and user-inclusive experience. Moreover, such practice raises awareness. People now know where to look for more information to share with their customers. And since they\u2019ve been involved personally, they feel pride and ownership, so they\u2019re keener to share that knowledge.</p>"},{"location":"docs-as-code/#how-to-fit-docs-as-code-in-a-developer-cycle","title":"How to fit Docs as Code in a developer cycle","text":"<p>It requires careful planning and coordination between technical writers and development teams. It warrants a lot of openness and transparency so that all stakeholders can follow the most efficient process and practices, which depend greatly on the company\u2019s resources.</p> <p>When do we write docs? When do we review them? Let\u2019s see how you can strap the entire documentation cycle to a developer one.</p> <p></p>"},{"location":"docs-as-code/#1-analysis","title":"1. Analysis","text":"<p>When developers are in their planning phase, technical writers can already start planning as well. They would identify which existing documents need changes and see how much new content will be needed. At this stage, they can already give a rough estimate of the time they will need to create a complete set of documents needed for the feature in question.</p> <p>By estimating the effort, the documentation team takes away the mysterious element of the \u201cit will be ready when it\u2019s ready\u201d attitude and can commit to concrete deadlines based on their own estimates.</p> <p>This practice protects both teams, the authors and the developers. The writers would never face a short and inhumane deadline anymore just because somebody remembered about documentation a couple of days before release day.</p>"},{"location":"docs-as-code/#2-design","title":"2. Design","text":"<p>During a design phase, technical writers can already start thinking about their own design \u2014 how they want to present information, what\u2019s the ideal user journey, what type of content is needed, how to map it to the existing content.</p>"},{"location":"docs-as-code/#3-implementation","title":"3. Implementation","text":"<p>Once the feature is being implemented, technical authors can start writing. For obvious reasons, this phase will not align perfectly with the implementation one but rather start at the very end of it, most likely during the bug-bashing phase, in which the authors can participate themselves to get familiar with the feature.</p>"},{"location":"docs-as-code/#4-testing","title":"4. Testing","text":"<p>Similar to developers, for technical writers, the testing phase is the time for last-minute improvements to their creations. That\u2019s exactly when they would start inviting stakeholders for peer review and go through a review cycle with them. Since we\u2019re propagating a Docs as Code approach, each document will have several reviewers and go through several review cycles simultaneously.</p>"},{"location":"docs-as-code/#5-release","title":"5. Release","text":"<p>Both documentation and code are released together on the same day. Since documentation hasn\u2019t been treated as an afterthought and technical writers were there every step of the cycle, documentation is well-planned, well-thought out, comprehensive and written with a user in mind.</p>"},{"location":"docs-as-code/#6-maintenance","title":"6. Maintenance","text":"<p>Again, documentation and code enter a maintenance phase together, and it looks similar for both. Anybody who spots an issue opens a ticket that\u2019s triaged by a team, prioritized and resolved. Likewise, anybody who sees an opportunity for an improvement opens a pull request and contributes to either code or documentation.</p>"},{"location":"docs-as-code/#key-takeaways","title":"Key takeaways","text":"<p>Overall, adopting a Docs as Code approach and incorporating peer review as an integral part of the development cycle enhances the quality of technical documentation, improves collaboration between teams and ensures documentation is given the attention it deserves throughout the entire software development process.</p>"},{"location":"intro-dita/","title":"Introduction to DITA","text":"<p>A markup language is a way of tagging content in a plain text file. The markup language you might be most familiar with is HTML, which is the fundamental markup language for the world wide web.</p> <p>DITA stands for Darwin Information Typing Architecture, which is an open standard originally created by IBM. IBM donated DITA to the Organization for the Advancement of Structured Information Standards (OASIS) in 2005.</p> <p>DITA is an XML-based, tool-independent way to create, organize, and manage content. DITA is built on:</p> <ul> <li>Topic-based authoring</li> <li>Separating content from formatting</li> <li>Minimalism</li> <li>Structured authoring concepts</li> </ul> <p>DITA is built based on the XML markup language. They both use angle brackets (&lt; and &gt;) to identify markup tags. In both languages, a forward slash identifies a close tag (<code>&lt;/title&gt;</code>).</p> <p>In HTML and XML, the tags can also have attributes (in the form attribute=\"value\") that provide further info about the tag.</p> XML<pre><code>&lt;note type=\"warning\"&gt;Keep your arms and legs inside the vehicle at all times.&lt;/note&gt;\n</code></pre> <p>There are two main differences between HTML and XML:</p> <ul> <li>HTML can be quite forgiving when you forget to close tags or put quotation marks around attribute values; XML is strict in requiring them.</li> <li>HTML uses a predefined set of tags (<code>&lt;body&gt;</code>, <code>&lt;p&gt;</code>, <code>&lt;span&gt;</code>, and so on). In XML, the tags are defined in a separate file and can be changed and added to by an information architect.</li> </ul> <p>DITA uses this tag naming feature in XML to define its own set of tag names or \u201celements\u201d. The DITA elements allow you to mark up content using names for things that make sense, such as <code>&lt;note&gt;</code> for notes, <code>&lt;section&gt;</code> for sections, <code>&lt;image&gt;</code> for images, and so on. However, because many HTML tag names make sense, their names are also used for DITA elements, such as <code>&lt;p&gt;</code> for paragraph, and <code>&lt;ul&gt;</code> and <code>&lt;ol&gt;</code> for unordered and ordered lists.</p> <p>One other difference in naming: in HTML, the outer-most or \u201croot\u201d tag is <code>&lt;html&gt;</code>. In DITA, the name of the root tag depends on the type of topic you\u2019re creating, such as <code>&lt;concept&gt;</code>, <code>&lt;task&gt;</code>, or <code>&lt;reference&gt;</code> (among others).</p>"},{"location":"intro-dita/#topics","title":"Topics","text":"<p>A DITA topic is a building block of content, the basic unit of authoring and reuse.</p> <p>In topic-based authoring, you create a number of individual topics, each of which addresses a single idea or answers a single question. These topics can then be used and reused in any order, in a number of different documents. In DITA, the topics are organized in maps, which are much like a table of contents; the map allows you to specify the order and the hierachy of the topics.</p> <p>To make your topics reusable:</p> <ul> <li>A topic should address a single idea or answer a single question.</li> <li>A topic should contain enough info to stand on its own.</li> <li>A topic should not assume any context. You should not make assumptions about what comes before or after the topic.</li> <li>A single file should contain a single topic.</li> </ul> <p>All DITA topics must have at least a title element and an id attribute for the root topic. The following is a valid DITA topic:</p> XML<pre><code>&lt;topic id=\"sample\"&gt;\n&lt;title&gt;Topic title goes here&lt;/tile&gt;\n&lt;/topic&gt;\n</code></pre>"},{"location":"intro-dita/#generic-topics","title":"Generic topics","text":"<p>All DITA topic types are based on a single generic topic type. The generic topic type is used as the basis to create the specific topic types. All topic types have this general structure:</p> XML<pre><code>&lt;topic id=\"sample\"&gt;\n&lt;title&gt;Topic title goes here&lt;/title&gt;\n&lt;shortdesc&gt;A short description&lt;/shortdesc&gt;\n&lt;body&gt;\n(Most of the elements go here.)\n\n&lt;/body&gt;\n&lt;/topic&gt; </code></pre> <ul> <li><code>&lt;title&gt;</code> is the only required element in a topic.</li> <li><code>&lt;body&gt;</code> contains the bulk of info in the topic. In the specific topic types the body element has a related name, such as <code>&lt;conbody&gt;</code> for concepts and <code>&lt;taskbody&gt;</code> for tasks.</li> </ul>"},{"location":"intro-dita/#concept-topics","title":"Concept topics","text":"<p>A concept topic answers the question \u201cWhy?\u201d It provides background information about a subject that the reader needs to know.</p> <p>Concepts usually contain paragraphs of text and lists, but also include notes, tables, and graphics needed to understand the ideas behind a particular subject.</p> <p>Common elements used in a concept topic include:</p> <ul> <li><code>&lt;conbody&gt;</code> (the body of the concept topic)</li> <li><code>&lt;p&gt;</code> (a paragraph)</li> <li><code>&lt;ul&gt;</code> (an unordered or bulleted list)</li> <li><code>&lt;ol&gt;</code> (an ordered or numbered list)</li> <li><code>&lt;li&gt;</code> (a list item inside a <code>&lt;ul&gt;</code> or <code>&lt;ol&gt;</code>)</li> <li><code>&lt;fig&gt;</code> (a figure, including an optional title)</li> <li><code>&lt;image&gt;</code> (a graphic inside a figure, or inline in text)</li> <li><code>&lt;section&gt;</code> (a subdivision in the topic, with an optional title)</li> </ul>"},{"location":"intro-dita/#task-topics","title":"Task topics","text":"<p>A task topic answers the question \"How do I?\" It includes step-by-step instructions to complete a procedure. DITA also allows step results, graphics, notes, and one level of substeps.</p> <p>Technical content is often heavy on tasks. DITA provides two task types:</p> <ul> <li>Strict task: requires all elements to appear in a specific order, only allows one <code>&lt;example&gt;</code> element, and has two very formal elements for steps (<code>&lt;steps&gt;</code> or <code>&lt;steps-unordered&gt;</code>).</li> <li>General task: also known as loose task, allows more flexibility in the order of the elements, allows multiple <code>&lt;example&gt;</code> elements, and allows a <code>&lt;step-informal&gt;</code> element for the steps, which can contain much more varied content.</li> </ul> <p>Strict tasks are appropriate for items that require step-by-step instructions; general tasks are useful for process overviews.</p> <p>Common elements used in a strict task topic include:</p> <ul> <li><code>&lt;taskbody&gt;</code> (the body of the task topic)</li> <li><code>&lt;steps&gt;</code> (the sequence of actions)</li> <li><code>&lt;step&gt;</code> (each individual action)</li> <li><code>&lt;cmd&gt;</code> (the action the user takes; this is a required element in a <code>&lt;step&gt;</code>)</li> <li><code>&lt;info&gt;</code> (additional info about the step)</li> <li><code>&lt;stepresult&gt;</code> (what happens after performing an action)</li> <li><code>&lt;stepxmp&gt;</code> (an example of how to do the step)</li> <li><code>&lt;example&gt;</code> (an example of how to do the entire task)</li> </ul>"},{"location":"intro-dita/#reference-topics","title":"Reference topics","text":"<p>A reference topic answers th question of \"What is it?\" A reference topic typically contains descriptive facts, such as the syntax of a command or API function call.</p> <p>Reference topics do not include steps or background info. Reference topics are similar to dictionary entries and provide facts only.</p> <p>Common elements used in a reference topic include:</p> <ul> <li><code>&lt;refbody&gt;</code> (the body of the reference topic)</li> <li><code>&lt;section&gt;</code> (a subdivision in the reference topic, with an optional title)</li> <li><code>&lt;table&gt;</code> (a table)</li> <li><code>&lt;fig&gt;</code> (a figure, including an optional title)</li> <li><code>&lt;properties&gt;</code> (a list of properties)</li> <li><code>&lt;refsyn&gt;</code> (a syntax diagram)</li> </ul>"},{"location":"intro-dita/#glossary-topics","title":"Glossary topics","text":"<p>A glossary entry topic answers the question \"What does this word or phrase mean?\" Glossary topics typically contain one term, along with one or more definitions.</p> <p>Common elements used in the glossary topic are:</p> <ul> <li><code>&lt;glossentry&gt;</code> (the glossary entry topic type)</li> <li><code>&lt;glossterm&gt;</code> (the word or phrase)</li> <li><code>&lt;glossdef&gt;</code> (the definition of the glossary term)</li> </ul>"},{"location":"intro-dita/#metadata","title":"Metadata","text":"<p>Metadata means \"beyond data\" or data about data. In XML, metadata lets you classify and manipulate info. In DITA, you can assign metadata to topics, individual elements, and more.</p> <p>You can assign metadata to DITA content in several different locations:</p> <ul> <li>At the topic level</li> <li>At the element level</li> <li>At the map file level (a map file lets you collect multiple topics to create a document, help system, and so on)</li> </ul> <p>At the topic level, DITA provides a <code>&lt;prolog&gt;</code> element in which you can store metadata for the entire topic. Here is an example of basic topic metadata:</p> XML<pre><code>&lt;topic id=\"xyz\"&gt;\n&lt;title&gt;Metadata example&lt;/title&gt;\n&lt;prolog&gt;\n&lt;author&gt;Sarah O\u2019Keefe, Scriptorium&lt;/author&gt;\n&lt;critdates&gt;\n&lt;created date=\"2015-05-01\"/&gt;\n&lt;/critdates&gt;\n&lt;/prolog&gt;\n&lt;body&gt;\n&lt;p&gt;Body content goes here&lt;/p&gt;\n&lt;/body&gt;\n&lt;/topic&gt;\n</code></pre> <p>Note</p> <p>Use the <code>&lt;prolog&gt;</code> metadata only for system information, such as the author and created/revised dataes.</p>"},{"location":"linux-basics/","title":"Basics","text":""},{"location":"linux-basics/#file-system-organization","title":"File System Organization","text":"<p>The files on a Linux system are arranged in what is called a hierachical directory structure. This means that they are organized in a tree-like pattern of directories (called folders in other systems), which may contain files and subdirectories.</p> <p>The first directory in the file system is called the root directory. Often we will see the file system represented like this:</p> <p></p> <p>One important difference between Windows and Unix-like operating systems such as Linux is that Linux does not employ the concept of drive letters. While Windows drive letters split the file system into a series of different trees (one for each device), Linux always has a single tree. Different storage devices may be different branches of the tree, but there is always just a single tree.</p> <p>Common commands used for navigation:</p> <ul> <li><code>pwd</code></li> <li><code>ls</code></li> <li><code>cd</code></li> </ul> <p>Important facts about shortcuts and file names:</p> <ul> <li>If we type <code>cd</code> followed by nothing, <code>cd</code> will change the working directory to our home directory.</li> <li>Typing <code>cd -</code> changes the working directory to the previous one.</li> <li>File names that begin with a period character <code>.</code> are hidden. This means that <code>ls</code> will not list them unless we say <code>ls -a</code>.</li> <li>File names in Linux are case-sensitive. The file names <code>File1</code> and <code>file1</code> refer to different files.</li> </ul>"},{"location":"linux-basics/#looking-around","title":"Looking Around","text":"<p>There are some tools that will come in handy during our journey.</p> <ul> <li><code>ls</code> (list files and directories)</li> <li><code>less</code> (view text files)</li> <li><code>file</code> (classify a file's contents)</li> </ul>"},{"location":"linux-basics/#ls","title":"<code>ls</code>","text":"Command Result <code>ls</code> List the files in the working directory. <code>ls /bin</code> List the files in the <code>/bin</code> directory (or any other directory we care to specify). <code>ls -l</code> List the files in the working directory in long format. <code>ls -l /etc /bin</code> List the files in the <code>/etc</code> and <code>/bin</code> directories in long format. <code>ls -la</code> List all files (even ones with names begining with a period character) in the parent of the working directory in long format. <p>A closer look at long format:</p> Text Only<pre><code>-rw-------   1 me       me            576 Apr 17  2019 weather.txt\ndrwxr-xr-x   6 me       me           1024 Oct  9  2019 web_page\n-rw-rw-r--   1 me       me         276480 Feb 11 20:41 web_site.tar\n-rw-------   1 me       me           5743 Dec 16  2018 xmas_file.txt\n\n----------     -------  -------  -------- ------------ -------------\n    |             |        |         |         |             |\n    |             |        |         |         |         File Name\n    |             |        |         |         |\n    |             |        |         |         +---  Modification Time\n    |             |        |         |\n    |             |        |         +-------------   Size (in bytes)\n    |             |        |\n    |             |        +-----------------------        Group\n    |             |\n    |             +--------------------------------        Owner\n    |\n    +----------------------------------------------   File Permissions\n</code></pre>"},{"location":"linux-basics/#less","title":"<code>less</code>","text":"<p>Once started, <code>less</code> will display a file one page at a time. We can use the Page Up and Page Down keys to move through the text file. To exit <code>less</code>, we type <code>q</code>. Here are some commands that <code>less</code> will accept:</p> Command Action Page Up or b Scroll back one page Page Down or space Scroll forward one page G Go to the end of the text file 1G Go to the beginning of the text file /characters Search forward in the text file for an occurrence of the specified characters n Repeat the previous search h Display a complete list less commands and options q Quit"},{"location":"linux-basics/#file","title":"<code>file</code>","text":"<p><code>file</code> will examine a file and tell us what kind of file it is.</p> File Type Description Viewable as text? ASCII text The name says it all yes Bourne-Again shell script text A bash script yes ELF 64-bit LSB executable An executable binary program no ELF 64-bit LSB shared object A shared library no GNU tar archive A tape archive file. A common way of storing groups of files. no, use tar tvf to view listing. gzip compressed data An archive compressed with gzip no HTML document text A web page yes JPEG image data A compressed JPEG image no PostScript document text A PostScript file yes Zip archive data An archive compressed with zip no"},{"location":"linux-basics/#a-guidied-tour-to-directories","title":"A Guidied Tour to Directories","text":"<p>To view a list of directories, click here.</p>"},{"location":"linux-basics/#manipulating-files","title":"Manipulating Files","text":""},{"location":"linux-basics/#wildcards","title":"Wildcards","text":"<p>Summary of wildcards and their meanings</p> <p></p> <p>For more info, see Wildcard.</p>"},{"location":"linux-basics/#io-redirection","title":"I/O Redirection","text":"<p>As we have seen, many commands such as <code>ls</code> print their output on the display. This does not have to be the case, however. By using some special notations we can redirect the output of many commands to files, devices, and even to the input of other commands.</p>"},{"location":"linux-basics/#standard-output","title":"Standard Output","text":"<p>To redirect standard output to a file, a \"&gt;\" character is used like this:</p> Bash<pre><code>ls &gt; file_list.txt\n</code></pre> <p>To have the new results append to the file, use \"&gt;&gt;\" like this:</p> Bash<pre><code>ls &gt;&gt; file_list.txt\n</code></pre>"},{"location":"linux-basics/#standard-input","title":"Standard Input","text":"<p>Many commands can accept input from a facility called standard input. By default, standard input gets its contents from the keyboard, but like standard output, it can be redirected. To redirect standard input from a file instead of the keyboard, the \"&lt;\" character is used like this:</p> Bash<pre><code>sort &lt; file_list.txt\n</code></pre> <p>We could redirect standard output to another file like this:</p> Bash<pre><code>sort &lt; file_list.txt &gt; sorted_file_list.txt\n</code></pre>"},{"location":"linux-basics/#pipelines","title":"Pipelines","text":"<p>With pipelines, the standard output of one command is fed into the standard input of another. Here is a very useful example:</p> Bash<pre><code>ls -l | less\n</code></pre>"},{"location":"linux-basics/#file-permissions","title":"File Permissions","text":"<p>The Unix-like operating systems, such as Linux differ from other computing systems in that they are not only multitasking but also multi-user.</p> <p>To see the permission settings for a file, use <code>ls</code> command.</p> Bash<pre><code>ls -l /bin/bash\n\n# output\n# -r-xr-xr-x  1 root  wheel  1296704 Jan  1  2020 /bin/bash\n</code></pre> <p>In the diagram below, we see how the first portion of the listing is interpreted. It consists of a character indicating the file type, followed by three sets of three characters that convey the reading, writing, and execution permission for the owner, group, and everybody else.</p> <p></p> <p>The <code>chmod</code> command is used to change the permission settings for a file or directory. To use it, we specify the desired permission settings and the file or files that we want to modify. In this example, we use octal notation to specify the permissions.</p> <p>It is easy to think of the permission settings as a series of bits (which is how the computer thinks about them). Here's how it works:</p> Text Only<pre><code>rwx rwx rwx = 111 111 111\nrw- rw- rw- = 110 110 110\nrwx --- --- = 111 000 000\n\nand so on...\n\nrwx = 111 in binary = 7\nrw- = 110 in binary = 6\nr-x = 101 in binary = 5\nr-- = 100 in binary = 4\n</code></pre> <p>For example, if we wanted to set <code>some_file</code> to have read and write permission for the owner, but wanted to keep the file private from others, we would:</p> Bash<pre><code>chmod 600 some_file\n</code></pre>"},{"location":"screenshots_tech_docs/","title":"Screenshots or not, for tech docs","text":"<p>Screenshots are a common and useful element in technical documentation. They can help readers visualize the interface, follow the steps, and verify the results of a task. However, screenshots also have some drawbacks, such as taking up space, requiring maintenance, and potentially revealing sensitive information. In this blog post, I will share some tips on when and how to capture screenshots for your technical documentation.</p>"},{"location":"screenshots_tech_docs/#when-to-capture","title":"When to capture","text":"<p>You should capture screenshots when they add value to your documentation. Some situations where screenshots are helpful are:</p> <ul> <li>When you introduce a new feature or concept that is not familiar to your audience.</li> <li>When you show a complex or important procedure that has multiple steps or options.</li> <li>When you illustrate a result or outcome that is not obvious or expected.</li> <li>When you compare or contrast different scenarios or alternatives.</li> </ul>"},{"location":"screenshots_tech_docs/#when-not-to-capture","title":"When not to capture","text":"<p>You should avoid taking screenshots when they are redundant, irrelevant, or distracting. Some situations where screenshots are not necessary are:</p> <ul> <li>When you describe a simple or common task that does not require visual guidance.</li> <li>When you repeat the same screenshot multiple times without any changes or annotations.</li> <li>When you include too many screenshots that clutter your page and make it hard to read.</li> <li>When you violate the privacy or security of your users or your organization.</li> </ul> <p>Another situation where you\u2019re better off without screenshots is attaching code samples. In such cases, it is often more effective to copy and paste the code directly into the document, rather than taking a screenshot of it.</p>"},{"location":"screenshots_tech_docs/#what-to-capture","title":"What to capture","text":"<p>The golden rule for creating quality screenshots is to capture the feature you\u2019re showing, with a bit of context included. You should capture only the essential elements in your screenshots. That being said, you should find the balance between including the details that provide context to the screenshot and adding too many distracting elements.</p> <p>Some guidelines for what to capture are:</p> <ul> <li>Crop your screenshots to show only the relevant parts of the interface. Remove any unnecessary menus, toolbars, icons, or windows.</li> <li>Focus on the action or result that you want to highlight. Use arrows, circles, boxes, or labels to draw attention to them.</li> <li>Use consistent sizes, resolutions, and formats for your screenshots. Follow the style and branding guidelines of your organization.</li> <li>Use high-quality images that are clear and legible. Avoid blurry, pixelated, or distorted screenshots.</li> </ul> <p>Here's an example screenshot obtained from the Azure SQL Database tutorial. You can see that sensitive information such as the server name, admin, and password is replaced with placeholders.</p> <p></p> <p>Note</p> <p>Making the user zoom in on large screenshots is not the best user experience, so it\u2019s better to make the spotlight of your screenshot immediately visible.</p>"},{"location":"screenshots_tech_docs/#what-not-to-capture","title":"What not to capture","text":"<p>You should avoid capturing any elements that are irrelevant, confusing, or harmful in your screenshots. For example, taking screenshots of your organization's internal information may lead to security breaches.</p> <p>Some guidelines for what not to capture are:</p> <ul> <li>Do not include any personal or confidential information in your screenshots. This includes names, email addresses, phone numbers, passwords, credit card numbers, etc. If you cannot avoid capturing such information, use a tool to blur or mask it.</li> <li>Do not include any errors, warnings, or notifications in your screenshots. This includes pop-ups, alerts, messages, or indicators that may imply something is wrong or incomplete. If you cannot avoid capturing such elements, use a tool to remove or edit them.</li> <li>Do not include any outdated or inconsistent information in your screenshots. This includes dates, times, versions, languages, currencies, etc. If you cannot avoid capturing such information, use a tool to update or standardize it.</li> </ul> <p>It\u2019s best to keep names out of screenshots in publicly available technical documentation. Besides real personal names, you should also obscure the names of servers, domains, and network folders from your technical documentation screenshots.</p> <p>Here's an example screenshot obtained from the Azure SQL Database tutorial. You can see that the surrounding elements are included in the screenshot, so that users can locate the feature.</p> <p></p> <p>Note</p> <p>If your product is prone to changes and requires a lot of\u00a0technical documentation maintenance, you should keep in mind that screens that are volatile aren\u2019t the best candidates for screenshots.</p>"},{"location":"screenshots_tech_docs/#what-to-highlight-in-screenshots","title":"What to highlight in screenshots","text":"<p>You should highlight the key points or benefits of your screenshots. Some ways to highlight them are:</p> <ul> <li>Use captions or titles to explain what your screenshots show and why they are important.</li> <li>Use colors or contrasts to emphasize the differences or similarities between your screenshots.</li> <li>Use animations or transitions to show the changes or progressions of your screenshots.</li> <li>Use stories or scenarios to provide context and relevance for your screenshots.</li> </ul>"},{"location":"screenshots_tech_docs/#checklist-for-screenshots","title":"Checklist for screenshots","text":"<p>Here are also some golden rules of thumb for screenshots usage in technical documentation:</p> <ul> <li> Size: Make screenshots no larger than 600 pixels wide.</li> <li> Scope: Limit the scope of a screenshot to just the portion of the UI that shows the action, plus enough surrounding details to help the user locate the item.</li> <li> Callouts: Use only arrows and boxes for callouts.</li> <li> File name: Create unique and meaningful file names to easily differentiate between screenshots, reflecting what has happened on the screenshots.</li> <li> Personal or private details: Make sure to mask, modify, or remove any personal identifiers, passwords, logins, or other information that could compromise security.</li> <li>  property: Use the  property to briefly describe the screenshot for visually-impaired readers. <p>By following these tips and checklists, you can create effective and engaging screenshots for your technical documentation.</p> <p>Happy documenting!</p>"},{"location":"set_up_blog/","title":"Set up your personal blog in minutes","text":"<p>In this blog post, I'll show you how to set up a personal blog in minutes using MkDocs and GitHub Pages. MkDocs is a simple documentation generator, and GitHub Pages is a free hosting service that lets you publish your website directly from a GitHub repository. By combining these two tools, you can create a fully-functional personal blog that is easy to set up and customize. Follow along and learn how to set up your own personal blog in just a few minutes!</p>"},{"location":"set_up_blog/#why-a-personal-blog","title":"Why a personal blog","text":"<p>Setting up a personal blog can be beneficial for a technical writer in several ways:</p> <ul> <li> <p>Establishing an online presence: A personal blog can help a technical writer establish an online presence and showcase their expertise in a particular field. This can help them build a personal brand and attract potential clients or employers.</p> </li> <li> <p>Demonstrating writing skills: A personal blog can be a platform for a technical writer to showcase their writing skills. By writing regularly on their blog, they can demonstrate their ability to communicate complex technical information in a clear and concise manner.</p> </li> <li> <p>Building a portfolio: A personal blog can also serve as a portfolio of a technical writer's work. They can use their blog to showcase their writing samples, case studies, and other relevant work examples.</p> </li> <li> <p>Sharing knowledge: By sharing their knowledge and expertise on their personal blog, a technical writer can establish themselves as a thought leader in their field. This can help them build credibility and attract new clients or job opportunities.</p> </li> <li> <p>Networking: A personal blog can also be a way for a technical writer to network with others in their field. By engaging with their readers and other bloggers, they can build relationships and expand their professional network.</p> </li> </ul>"},{"location":"set_up_blog/#set-up-mkdocs","title":"Set up MkDocs","text":"<p>To use MkDocs, you need to install pip. You can find instructions on how to install pip here. If you have pip, make sure to update it, and then install MkDocs. While you install MkDocs, you should also pick a theme (check out the options here). In the example, we picked the mkdocs-material theme.</p> <p>Run the following code to install <code>pip</code>, <code>MkDocs</code> and theme <code>mkdocs-material</code>:</p> Bash<pre><code>pip install --upgrade pip\npip install mkdocs\npip install mkdocs-material\n</code></pre> <p>Now you\u2019re ready to create your documentation. Run the command below, but replace <code>PROJECT_NAME</code> with whatever your project name is:</p> Bash<pre><code>mkdocs new PROJECT_NAME # Create a new MkDocs project\ncd PROJECT_NAME # Redirect to the directory where the project is created\n</code></pre> <p>You should see a file named mkdocs.yaml and a folder named docs. The folder will have a single markdown file, that is, index.md.</p> <p>To preview the docs layout, run <code>mkdocs serve</code> and then Point your browser to <code>http://127.0.0.1:8000/</code>. If everything's up, information similar to the following appears:</p> <p></p> <p>Then, open <code>mkdocs.yaml</code>, and you can see the following:</p> Bash<pre><code>site_name: My Docs\n</code></pre> <p>We\u2019re going to edit this file. First, we\u2019ll create a generic outline; feel free to fill in the placeholder variables.</p> YAML<pre><code>site_name: NAME\nnav:\n- Home: index.md\n- Page2: page2.md\n- Section1:\n- Subpage1: subpage1.md\n- Subpage2: subpage2.md\ntheme:\nname: THEME_DOWNLOADED\n</code></pre> <p>For example, assume that I want to set up a personal blog, sharing daily insights and showcasing writing samples. Then, this is what I would like to include in <code>mkdocs.yml</code>:</p> YAML<pre><code>site_name: My Blog\nnav:\n- Home: index.md\n- About: about.md\n- Writing:\n- \"Tech\": writing/article1.md\n- \"Rambling\": writing/article2.md\n- Learn:\n- \"Coding\": learn/article1.md\n- \"AI\": learn/article2.md\ntheme:\nname: material\n</code></pre> <p>Note that all the folders and directories included in <code>mkdocs.yaml</code> must be in the <code>docs</code> directory. This is how my structure would look like:</p> Text Only<pre><code>PROJECT_NAME/\n    docs/\n        index.md\n        about.md\n        writing/\n             article1.md\n             article2.md\n        learn/\n             article1.md\n             article2.md\n    mkdocs.yaml\n</code></pre>"},{"location":"set_up_blog/#build-your-site","title":"Build your site","text":"<p>After all steps above are completed, you can use the following code to build a static site from Markdown files:</p> Bash<pre><code>mkdocs build\n</code></pre> <p>Next, you can proceed to the next step, hosting the site on GitHub Pages.</p>"},{"location":"set_up_blog/#lastly-deploy-your-site","title":"Lastly, deploy your site","text":"<p>To end, we\u2019ll host our documentation on GitHub Pages. Simply run <code>mkdocs gh-deploy</code>. It should have created a new branch in your repository that will host your site at <code>USERNAME.github.io/REPOSITORY_NAME</code>.</p> <p>If you want to learn more about MkDocs options, visit their official website.</p>"},{"location":"template-based-doc-generation/","title":"Use Python to generate docs based on templates","text":"<p>In today's digital age, document generation plays a crucial role in various industries and sectors. The efficiency and accuracy of document generation significantly impact business processes, productivity, and customer satisfaction.</p> <p>One powerful approach to streamlining document creation is template-based document generation.</p> <p>Templates provide a structured framework that allows for consistent formatting and content placement. They offer numerous advantages, including time-saving, standardization, and branding consistency. With templates, businesses can easily generate personalized documents by replacing placeholders with relevant data.</p> <p>However, the potential of template-based document generation doesn't end there. By incorporating Natural Language Processing (NLP) and Artificial Intelligence (AI) techniques, we can take document automation to a whole new level.</p> <p>NLP enables intelligent analysis and understanding of the text, while AI provides advanced capabilities such as data extraction, content generation, and automated decision-making. Together, NLP and AI can enhance document generation by automating data entry, extracting valuable insights, and generating tailored content based on user preferences.</p> <p>In this blog, we will explore the power of template-based document generation, delve into the benefits it offers, and discover the exciting possibilities when NLP and AI are integrated into the process. Let's see the potential of these technologies to revolutionize the way we create, manage, and utilize documents.</p>"},{"location":"template-based-doc-generation/#template-based-document-generation","title":"Template-Based Document Generation","text":"<p>The template-based approach simplifies and streamlines document generation by providing a structured framework. Templates serve as blueprints, outlining the layout, formatting, and placeholders for dynamic content. When creating documents, we replace these placeholders with actual data, resulting in customized and consistent outputs.</p> <p>To implement template-based document generation, we design templates using familiar applications like Microsoft Word, HTML, or PDF.</p> <p>These templates define the document's structure, including headers, footers, tables, and text formatting. And then, we insert placeholders, marked by specific tags, at the positions where dynamic content will be inserted.</p>"},{"location":"template-based-doc-generation/#benefits-of-using-templates-for-document-generation","title":"Benefits of Using Templates for Document Generation","text":"<p>Using templates can offer a wide range of benefits. First, they save time and effort. Instead of starting from scratch for each document, we can reuse templates, eliminating repetitive work. Templates also ensure consistency across documents, maintaining a professional image for your business.</p> <p>With predefined placeholders, it becomes easy to insert data programmatically, automating the process. This reduces the chances of errors and enables quick generation of documents, especially when dealing with large volumes.</p>"},{"location":"template-based-doc-generation/#template-formats","title":"Template Formats","text":"<p>Template formats vary depending on the intended use and the application employed. Microsoft Word templates (DOCX) are widely used for their flexibility and rich formatting capabilities.</p> <p>HTML templates offer compatibility across different platforms and can be rendered in web browsers or converted to PDF. PDF templates are excellent for maintaining document integrity and ensuring consistent appearance across devices and operating systems.</p> <p>Here's a simple example in Python, using the Docxtemplater library, to demonstrate template-based document generation:</p> Python<pre><code>from docxtpl import DocxTemplate\n# Load the template file\ntemplate = DocxTemplate('invoice_template.docx')\n# Define data for the document\ndata = {\n'customer_name': 'John Doe',\n'order_number': '12345',\n'total_amount': '$100',\n}\n# Render the template with the data\ntemplate.render(data)\n# Save the generated document\ntemplate.save('generated_invoice.docx')\n</code></pre> <p>In this example, we load an invoice template created in Microsoft Word (DOCX format). We populate the template with data such as customer name, order number, and total amount.</p> <p>Finally, we render the template with the data and save the generated invoice as a new document.</p> <p>And now, let\u2019s explore how incorporating Natural Language Processing (NLP) and Artificial Intelligence (AI) into template-based document generation can further enhance its capabilities, opening up exciting possibilities for automation and intelligent document processing.</p>"},{"location":"template-based-doc-generation/#natural-language-processing-nlp-in-document-generation","title":"Natural Language Processing (NLP) in Document Generation","text":"<p>Natural Language Processing (NLP) is a branch of AI that focuses on the interaction between computers and human language. It enables computers to understand, interpret, and generate human language, opening up exciting possibilities in document generation. NLP has various applications that enhance the document creation process.</p> <p>NLP also facilitates language translation in the document generation process. It uses algorithms and models to understand, process, and translate human language. Here's a simplified explanation with basic coding:</p> <ol> <li> <p>Language identification:</p> <ul> <li>NLP can automatically detect the language of a document using libraries like langid.py.</li> <li>Example code snippet:</li> </ul> Python<pre><code>import langid\ntext = \"This is a sample text in English.\"\ndetected_lang = langid.classify(text)\nprint(detected_lang)\n</code></pre> </li> <li> <p>Machine translation:</p> <ul> <li>NLP models and translation APIs like Google Translate enable automated translation.</li> <li>Example code snippet using the Google Translate API:</li> </ul> Python<pre><code>from googletrans import Translator\ntranslator = Translator()\ntext = \"Hello, how are you?\"\ntranslated_text = translator.translate(text, dest='fr').text\nprint(translated_text)\n</code></pre> </li> <li> <p>Post-editing and quality assessment:</p> <ul> <li>NLP tools like LanguageTool or spaCy can help identify errors and improve machine-translated content.</li> <li>Example code snippet using LanguageTool:</li> </ul> Python<pre><code>from language_tool_python import LanguageTool\ntool = LanguageTool('en-US')\ntext = \"This are sample sentences.\"\nerrors = tool.check(text)\nfor error in errors:\nprint(error)\n</code></pre> </li> </ol> <p>By leveraging NLP techniques and tools, businesses can automate language translation in document generation, ensuring accurate and localized content for diverse audiences.</p>"},{"location":"template-based-doc-generation/#the-power-of-ai-in-document-classification-and-content-generation","title":"The Power of AI In Document Classification and Content Generation","text":"<p>In document generation, AI technologies such as machine learning, natural language processing, and computer vision play a vital role. Machine learning algorithms can be trained to recognize patterns in data, enabling AI systems to understand document structures and extract relevant information.</p> <p>AI-driven data extraction and intelligent content organization are crucial aspects of document generation. AI algorithms can automatically extract data from diverse sources such as forms, invoices, or receipts, reducing the need for manual data entry. This not only saves time but also minimizes the risk of errors.</p> <p>Additionally, AI enables an intelligent content organization, where documents are categorized, tagged, and indexed automatically. AI systems can analyze the content and assign appropriate metadata, making it easier to search, retrieve, and manage documents efficiently.</p> <p>AI-powered document classification and automated content generation are game-changers in document generation. AI algorithms can classify documents based on their content, enabling quick categorization and organization of large document repositories. This helps streamline document management and retrieval processes.</p> <p>Moreover, AI can automate content generation by leveraging machine learning models. For example, AI systems can learn from existing documents to generate new content with similar patterns, such as contract clauses or legal agreements. This not only speeds up the document creation process but also ensures consistency and adherence to predefined standards.</p> <p>Here's a simplified Python code example showcasing AI-driven data extraction using the Textract service from AWS:</p> Python<pre><code>import boto3\n# Initialize the Textract client\ntextract_client = boto3.client('textract')\n# Specify the document to extract data from\ndocument_path = 'invoice.pdf'\n# Perform document text extraction using Textract\nresponse = textract_client.detect_document_text(Document={'S3Object': {'Bucket': 'your-bucket', 'Name': document_path}})\n# Retrieve the extracted text\nextracted_text = response['Blocks'][0]['Text']\n# Print the extracted text\nprint('Extracted Text:', extracted_text)\n</code></pre> <p>In this example, we use the Textract service from AWS to extract text from a document (in this case, an invoice in PDF format). The Textract API analyzes the document and returns the extracted text as a response. This AI-driven data extraction eliminates the need for manual data entry and allows for seamless integration into document generation workflows.</p>"},{"location":"template-based-doc-generation/#python-libraries-for-template-based-document-generation","title":"Python Libraries for Template-Based Document Generation","text":"<p>Python offers powerful libraries that simplify template-based document generation. Two popular libraries are Docxtemplater and Jinja2.</p> <p>Docxtemplater allows the creation and manipulation of Microsoft Word documents (DOCX format) with placeholders, while Jinja2 provides a flexible templating engine for generating various types of documents, including HTML, XML, and text files.</p> <p>Creating and customizing templates using Python is straightforward. With Docxtemplater, you can load an existing Word document template, define placeholders, and programmatically replace them with actual data.</p> <p>Jinja2 provides a template engine where you can define templates with dynamic sections and variables. These templates can be rendered with data to generate final documents.</p> <p>Here's a simple example using Docxtemplater to create a customized invoice:</p> Python<pre><code>from docxtpl import DocxTemplate\n# Load the invoice template\ntemplate = DocxTemplate('invoice_template.docx')\n# Define data for the invoice\ndata = {\n'customer_name': 'John Doe',\n'invoice_number': 'INV-001',\n'total_amount': '$100',\n}\n# Render the template with the data\ntemplate.render(data)\n# Save the generated invoice\ntemplate.save('generated_invoice.docx')\n</code></pre> <p>In this code snippet, we load an invoice template created in Microsoft Word (DOCX format) using Docxtemplater. We define the data to be inserted into the template, such as customer name, invoice number, and total amount. Then, we render the template with the provided data and save the generated invoice as a new document.</p> <p>Python libraries like NLTK (Natural Language Toolkit), SpaCy, and TensorFlow provide NLP and AI capabilities that can be integrated into the document generation process. NLTK offers a wide range of NLP functionalities, including text tokenization, part-of-speech tagging, and sentiment analysis.</p> <p>SpaCy provides advanced NLP features such as named entity recognition and dependency parsing. TensorFlow, on the other hand, is a powerful machine-learning framework that can be used for tasks like text classification or content generation.</p> <p>By incorporating these libraries into the document generation workflow, you can leverage NLP and AI techniques to enhance the generated documents. For example, you can use NLTK to analyze customer feedback and extract meaningful insights or employ SpaCy to identify and categorize entities mentioned in the documents. TensorFlow can be utilized to train models that generate customized content based on specific criteria or patterns.</p>"},{"location":"template-based-doc-generation/#use-cases-and-real-world-examples","title":"Use Cases and Real-World Examples","text":"<p>In the legal sector, it simplifies contract creation by automating the insertion of client-specific details into standardized templates.</p> <p>In healthcare, it aids in generating medical reports and patient records with consistent formatting.</p> <p>Businesses can leverage templates for creating invoices, sales proposals, or marketing materials, ensuring brand consistency and saving time.</p> <p>AI algorithms can be trained to generate personalized customer communication, such as bank statements or loan agreements, based on individual preferences and data.</p> <p>In the publishing domain, NLP can automate the creation of book summaries or generate metadata for digital content. AI-powered content generation can assist content creators by suggesting topic ideas, generating drafts, or summarizing research articles.</p> <p>The benefits and outcomes of template-based document generation with NLP and AI are remarkable. Businesses experience increased productivity as manual tasks are automated, allowing employees to focus on more strategic activities. Time savings are significant, particularly when dealing with large volumes of documents. Accuracy improves as AI algorithms reduce errors and extract relevant information precisely.</p> <p>Moreover, template-based document generation ensures consistency in document formatting and branding, enhancing the professional image of businesses. The integration of NLP and AI capabilities enables intelligent analysis, extraction, and generation of content, leading to improved decision-making, personalized customer experiences, and enhanced operational efficiency.</p>"},{"location":"types-of-docs/","title":"Types of Docs","text":"<p>Di\u00e1taxis identifies four modes of documentation - tutorials, how-to guides, technical reference and explanation. It derives its structure from the relationship between them.</p> <p>In Di\u00e1taxis, each of these modes (or types) corresponds to a different user need. Each fulfils a different purpose and requires a different approach to its creation.</p> <p></p>"},{"location":"types-of-docs/#tutorials","title":"Tutorials","text":"<p>Tutorials are lessons that take the reader by hand through a series of steps to complete a project of some kind. Tutorials must be learning-oriented.</p> <p>A tutorial must help a beginner achieve basic competence with a product, so that they can go on to use the product for their own purpose.</p> <p>A lesson entails a relationship between a teacher and a pupil. For documentation, a lesson is a learning experience. If you are not providing your learner with a learning experience, your tutorial is not doing the job it needs to.</p>"},{"location":"types-of-docs/#write-a-good-tutorial","title":"Write a Good Tutorial","text":"<p>There are some key aspects for writing a good tutorial:</p> <ul> <li>Do not try to teach. Allow the user to learn. Give your learner things to do, through which they can learn. As you lead the pupil through the steps you have devised, have them use the tools and perform the operations they'll need to become familiar with, building up from the simplest ones at the start to more complex ones.</li> <li>Get the learner started, not to turn them into an expert. Don\u2019t ever be embarrassed to start right at the beginning: a user can skim rapidly over what\u2019s unnecessary, but if they need something and it\u2019s not there, you risk losing them altogether.</li> <li>Provide a complete picture before they start. Allow the learn to form an idea of what they will achieve right from the start.</li> <li>Ensure that the tutorial works reliably. One of your jobs as a tutor is to inspire the beginner's confidence. Confidence can only be built up layer by layer, but is easily shaken. The single most important requirement is that what you ask the beginner to do must work.</li> <li>Ensure the user sees results immediately. Your learner is probably doing new and strange things that they don't understand. Don't make them do too many before they see a result from their actions. As far as possible, the effect of every action should be clear to them as soon as possible. The relation of cause and effect should be evident.</li> <li>Make your tutorial repeatable. The users of your tutorial will have different levels of skill and understanding. They might also be using different tools and operating systems and you can't rely on them having the same resources or environment. This makes repeatable reliability extremely hard to achieve, and yet, your tutorial should work for all users, every time.</li> <li>Describe concret steps, not abstract concepts or discussion. All learning proceeds from the particular and concrete to the general and abstract.</li> <li>Offer only minimum, necessary, explanation. If the learner does not need an explanation in order to complete the tutorial, do not explain.</li> <li>Ignore options and alternatives. Your job is to guide the learner to a successful conclusion. There may be many diversions along the way, such as different options for the command, different ways to use the API, different approaches to the task you are describing - ignore them. Your guidance needs to remain focused on what's required to reach the conclusion, and everything else can be left for another time.</li> </ul>"},{"location":"types-of-docs/#language-of-tutorials","title":"Language of Tutorials","text":"<p>In this tutorial, you will...</p> <p>First, do x. Now do y. Now that you have done y, do z.</p> <p>We must always do x before we do y because\u2026 (see Explanation for more details).</p> <p>The output should look something like this...</p> <p>Notice that... Remember that...</p> <p>You have built a secure, three-layer hylomorphic stasis engine...</p>"},{"location":"types-of-docs/#how-to-guides","title":"How-to Guides","text":"<p>How-to guides are goal-oriented, which can be thought of as recipes, directions that guide the reader through the steps to achieve a specific end.</p> <p>The list of how-to guides in your documentation helps frame the picture of what your product can actually do. A rich list of how-to guides is an encouraging suggestion of a product's capabilities.</p> <p>Consider a recipe, an excellent model for a how-to guide. A recipe clearly defines what will be achieved by following it, and addresses a specific question (How do I make...? or What can I make with...?).</p> <p>Following a recipe requires at least basic competence. Someone who has never cooked before should not be expected to follow a recipe with success, so a recipe is not a substitute for a cooking lesson.</p>"},{"location":"types-of-docs/#write-a-good-how-to-guide","title":"Write a Good How-to Guide","text":"<p>There are some key aspects for writing a good how-to guide:</p> <ul> <li>Describe a sequence of actions. Unlike a tutorial, you don't have to start at the beginning of the whole story and take your reader right to the end. Most likely, your user will also be in the middle of something, so you only need to provide a starting point that they know how to reach, and a conclusion that actually answers a real question.</li> <li>Solve a problem. The problem or task is the concern of a how-to guide: stick to that practical goal. Anything else that's added - unnecessary explanation, for example, distracts both you and the user and dilutes the useful power of the guide.</li> <li>Don't explain concepts. An explanation doesn\u2019t show you how to do something - so a how-to guide should not try to explain things. If explanations are important, link to them.</li> <li>Omit the necessary. In how-to guides, practical usability is more helpful than completeness. Whereas a tutorial needs to be a complete, end-to-end guide, a how-to guide does not. It should start and end in some reasonable, meaningful place, and require the reader to join it up to their own work.</li> <li>Pay attention to naming.</li> <li>Good: How to integrate application performance monitoring</li> <li>Bad: Integrating application performance monitoring (maybe the document is about how to decide whether you should, not about how to do it)</li> <li>Very bad: Application performance monitoring (maybe it\u2019s about how - but maybe it\u2019s about whether, or even just an explanation of what it is)</li> </ul>"},{"location":"types-of-docs/#language-of-how-to-guides","title":"Language of How-to Guides","text":"<p>This guide shows you how to...</p> <p>If you want x, do y. To achieve w, do z.</p> <p>Refer to the x reference guide for a full list of options.</p>"},{"location":"types-of-docs/#reference","title":"Reference","text":"<p>Reference guides are technical descriptions of the machinery and how to operate it. Reference material is information-oriented.</p> <p>The only purpose of a reference guide is to describe, as succinctly as possible, and in an orderly way. Whereas the content of tutorials and how-to guides are led by the needs of the user, reference material is led by the product it describes.</p> <p>In the case of software, reference guides describe the software itself, APIs, classes, functions and so on - and how to use them.</p> <p>Your users need reference material because they need truth and certainty. In the case of reference material, what you're seeking is information - accurate, up-to-date, comprehensive information.</p>"},{"location":"types-of-docs/#write-a-good-reference-guide","title":"Write a Good Reference Guide","text":"<p>There are some key aspects for writing a good reference guide:</p> <ul> <li>Respect the structure of the machinery. The structure of reference documentation should mirror the structure of the product.</li> <li>Be consistent, in structure, language, terminology, tone.</li> <li>Do nothing but describe. Technical reference has one job: to describe, and to do that clearly, accurately and comprehensively. Doing anything else - explaining, discussing, instructing, speculating - gets in the way of that job, and makes it harder for the reader to find the information they need.</li> <li>Provide examples. Examples are valuable ways of providing illustration that helps readers understand reference, without becoming distracted from the job of describing.</li> <li>Be accurate. These descriptions must be accurate and kept up-to-date. Any discrepancy between the machinery and your description of it will inevitably lead a user astray.</li> </ul>"},{"location":"types-of-docs/#language-of-reference-guides","title":"Language of Reference Guides","text":"<p>X is an example of y. W needs to be initialised using z. This option does that.     State facts about the machinery and its behavior.</p> <p>Sub-commands are: a, b, c, d, e, f.     List commands, options, operations, features, flags, limitations, error messages, etc.</p> <p>You must use a. You must not apply b unless c. Never d.     Provide warnings where appropriate.</p>"},{"location":"types-of-docs/#explanation","title":"Explanation","text":"<p>Explanation is understanding-oriented, which clarifies and illuminates a particular topic.</p> <p>In the case of explanation, it\u2019s useful to have a real or imagined why question to serve as a prompt. Otherwise, you simply have to draw some lines that mark out a reasonable area and be satisfied with that.</p>"},{"location":"types-of-docs/#write-good-explanation","title":"Write Good Explanation","text":"<p>There are some key aspects for writing good explanation:</p> <ul> <li>Make connections. When writing explanation you are helping to weave a web of understanding for your readers. Make connections to other things, even to things outside the immediate topic, if that helps.</li> <li>Provide context. Provide background and context in your explanation: explain why things are so - design decisions, historical reasons, technical constraints - draw implications, mention specific examples.</li> <li>Talk about the subject. Explanation guides are about a topic in the sense that they are around it. Even the names of your explanation guides should reflect this; you should be able to place an implicit (or even explicit) about in front of each title. For example: About user authentication, or About database connection policies.</li> <li>Discuss alternatives and opinions. Explanation can consider alternatives, counter-examples or multiple different approaches to the same question. You\u2019re not giving instruction or describing facts - you\u2019re opening up the topic for consideration. It helps to think of explanation as discussion: discussions can even consider and weigh up contrary opinions.</li> </ul>"},{"location":"types-of-docs/#language-of-explanation","title":"Language of Explanation","text":"<p>The reason for x is because historically, y...     Explain.</p> <p>W is better than z, because...     Offer judgements and even opinions where appropriate..</p> <p>An x in system y is analogous to a w in system z. However, ...     Provide context that helps the reader.</p> <p>Some users prefer w (because z). This can be a good approach, but...     Weigh up alternatives.</p> <p>An x interacts with a y as follows:...     Unfold the machinery\u2019s internal secrets, to help understand why something does what it does.</p>"},{"location":"types-of-docs/#takeaways","title":"Takeaways","text":"<p>The following table provides a summary of these doc types.</p> <p></p> <p>Compass for structuring documentation</p> <p></p>"},{"location":"vectordb-vs-graphdb/","title":"Vector Databases vs. Graph Databases: A Comprehensive Comparison","text":"<p>In this blog post, we will compare vector databases and graph databases, two types of databases that are widely used in various applications that involve data analysis and machine learning. We will explain what each database is, how they work, and what are their distinctive use cases. We will also discuss how vector databases and graph databases can empower large language models (LLMs), which are a buzzword since the AI revolution caused by ChatGPT.</p> <p>If you\u2019d like a summary of the discussed concepts, see the table below.</p> Item Vector Database Graph Database Data Structure Based on high-dimensional vectors, suitable for handling high-dimensional data such as photos and movies. Built on nodes and edges, suitable for processing data with complicated relationships. Data Type Appropriate for analyzing patterns in the data, but may not be ideal for analyzing relationships between entities. Better suited for analyzing relationships between entities and complex networks. Query Ideal for similarity searches, but may not be as efficient for analyzing relationships between entities. More efficient for analyzing relationships between entities and complex networks, but may not be as efficient for similarity searches. Scalability Highly scalable due to distributed architecture. May not be as scalable as vector databases, but provides more flexibility and functionality. Performance Provides fast similarity searches. Provides fast queries involving relationships. Business Needs and Use Cases The choice depends on specific business needs and use cases. The choice depends on specific business needs and use cases."},{"location":"vectordb-vs-graphdb/#what-is-a-vector-database","title":"What is a Vector Database?","text":"<p>A vector database is a type of database that stores data as high-dimensional vectors, which are mathematical representations of features or attributes. Each vector has a certain number of dimensions, which can range from tens to thousands, depending on the complexity and granularity of the data. The vectors are usually generated by applying some kind of transformation or embedding function to the raw data, such as text, images, audio, video, and others. The embedding function can be based on various methods, such as machine learning models, word embeddings, feature extraction algorithms.</p>"},{"location":"vectordb-vs-graphdb/#why-you-need-a-vector-database","title":"Why You Need a Vector Database","text":"<p>The main advantage of a vector database is that it allows for fast and accurate similarity search and retrieval of data based on their vector distance or similarity. This means that instead of using traditional methods of querying databases based on exact matches or predefined criteria, you can use a vector database to find the most similar or relevant data based on their semantic or contextual meaning. For example, you can use a vector database to:</p> <ul> <li>Find images that are similar to a given image based on their visual content and style.</li> <li>Find documents that are similar to a given document based on their topic and sentiment.</li> <li>Find products that are similar to a given product based on their features and ratings.</li> </ul> <p>To perform similarity search and retrieval in a vector database, you need to use a query vector that represents your desired information or criteria. The query vector can be either derived from the same type of data as the stored vectors (e.g., using an image as a query for an image database), or from different types of data (e.g., using text as a query for an image database). Then, you need to use a similarity measure that calculates how close or distant two vectors are in the vector space. The similarity measure can be based on various metrics, such as cosine similarity, euclidean distance, hamming distance, and jaccard index. The result of the similarity search and retrieval is usually a ranked list of vectors that have the highest similarity scores with the query vector. You can then access the corresponding raw data associated with each vector from the original source or index.</p>"},{"location":"vectordb-vs-graphdb/#what-is-a-graph-database","title":"What is a Graph Database?","text":"<p>A graph database is a type of database that stores data as nodes and edges, which are entities and relationships. Each node can have one or more properties or attributes, such as name, type, value, etc. Each edge can have one or more labels or types, such as <code>friend_of</code>, <code>belongs_to</code>, etc. The nodes and edges form a graph structure that represents the data and its connections.</p>"},{"location":"vectordb-vs-graphdb/#why-you-need-a-graph-database","title":"Why You Need a Graph Database","text":"<p>The main advantage of a graph database is that it allows for efficient and flexible analysis of data with complex relationships and networks. This means that instead of using traditional methods of querying databases based on tables and joins, you can use a graph database to find the most relevant or interesting data based on their connections and patterns. For example, you can use a graph database to:</p> <ul> <li>Find people who are connected to a given person through some criteria, such as friends, colleagues, family members, etc.</li> <li>Find products that are related to a given product through some criteria, such as categories, reviews, purchases, etc.</li> <li>Find events that are influenced by or influence other events through some criteria, such as causes, effects, dependencies, etc.</li> </ul> <p>To perform analysis in a graph database, you need to use a query language that supports graph operations and traversal. The query language can be based on various standards or frameworks, such as Cypher, Gremlin, SPARQL, etc. The result of the analysis is usually a subgraph or a set of nodes and edges that match the query criteria. You can then access the corresponding properties and attributes associated with each node and edge from the original source or index.</p>"},{"location":"vectordb-vs-graphdb/#use-cases-comparison","title":"Use Cases Comparison","text":"<p>The choice between vector databases and graph databases depends on specific business needs and use cases. Both types of databases have their own strengths and weaknesses when it comes to handling different types of data and queries. Here are some examples of use cases where one type of database may be more suitable than the other:</p> <ul> <li>Image search: Vector databases are more suitable for image search applications because they can store and retrieve images based on their features and similarity. Graph databases may not be as efficient for image search because they may not capture the nuances and variations of image content and style.</li> <li>Fraud detection: Graph databases are more suitable for fraud detection applications because they can analyze relationships between entities like users, accounts, and transactions. Vector databases may not be as efficient for fraud detection because they may not capture the complexity and dynamics of fraud patterns and networks.</li> <li>Recommendation systems: Both vector databases and graph databases can be used for recommendation systems, depending on the type of data and criteria. Vector databases can be used to recommend items based on their similarity or relevance to a given item or user. Graph databases can be used to recommend items based on their relationships or connections to a given item or user.</li> <li>Natural language processing: Both vector databases and graph databases can be used for natural language processing applications, depending on the type of data and task. Vector databases can be used to store and retrieve text based on their meaning and sentiment. Graph databases can be used to store and analyze text based on their structure and syntax.</li> </ul>"},{"location":"vectordb-vs-graphdb/#how-they-can-empower-llms","title":"How They Can Empower LLMs","text":"<p>Large language models (LLMs) are a type of machine learning model that can generate natural language text based on some input or context. LLMs are trained on massive amounts of text data from various sources, such as books, articles, websites, etc. LLMs can perform various tasks, such as text summarization, text generation, text completion, text translation, etc.</p> <p>However, LLMs often face challenges such as generating inaccurate or irrelevant information; lacking factual consistency or common sense; repeating or contradicting themselves; being biased or offensive. These challenges can be attributed to the limitations of the training data and the model architecture. One way to overcome these challenges is to use vector databases and graph databases to enhance the LLMs' capabilities and performance.</p> <p>Vector databases can help LLMs by providing them with high-dimensional vectors that represent the semantic or contextual information of the input or output text. These vectors can be used to guide the LLMs to generate more relevant and coherent text based on the vector similarity or distance. For example, a vector database can help an LLM to generate a summary of a document by providing it with a vector that represents the main idea or theme of the document.</p> <p>Graph databases can help LLMs by providing them with nodes and edges that represent the factual or logical information of the input or output text. These nodes and edges can be used to guide the LLMs to generate more accurate and consistent text based on the graph structure or traversal. For example, a graph database can help an LLM to generate a biography of a person by providing it with a subgraph that represents the person's attributes and relationships.</p>"},{"location":"vectordb-vs-graphdb/#conclusion","title":"Conclusion","text":"<p>In this blog post, we have compared vector databases and graph databases, two types of databases that are widely used in various applications that involve data analysis and machine learning. We have explained what each database is, how they work, and what are their distinctive use cases. We have also discussed how vector databases and graph databases can empower large language models (LLMs), which are a buzzword since the AI revolution caused by ChatGPT.</p> <p>We hope that this blog post has helped you understand the differences and similarities between vector databases and graph databases, and how they can benefit your business needs and use cases.</p>"},{"location":"year_review_2022/","title":"Year in review 2022","text":"<p>In many ways, 2022 seemed like an extension of 2021 \u2014 the stories all too familiar, only now with added intensity and uncertainty. COVID-19 curbs got harsher. The climate crisis became worse. Episodes of violence against women more shocking. But a cute Olympics mascot, a handful of remarkable performances by female athletes, and the metaverse bubble served as welcome distractions. For me, personally, the year 2022 might look and feel a bit different.</p>"},{"location":"year_review_2022/#work","title":"Work","text":"<p>In May, 2022, I left my previous job as a translator as I found translating tech doc made me passive and it had limited value to improve my tech skills since I was forced to deal with the texts of all types, from storage to databases, and to containers. This might be helpful if my goal was to become a generalist. However, as a language student specialized in zero, I seek to become a T-shaped talent who deep dives into a specific field first and also has basic knowledge in other fields. That\u2019s basically the reason why I decided to change my job and started to work for the current startup as a technical writer.</p> <p>Of course, I encountered a handful of challenges and uncertainties in the new position. For example, when I first came to the company, I didn\u2019t know what git was, how to manage content and collaborate with other colleagues using PRs, what documentation as code was, and let alone how HTML, CSS, JS work. Besides, the management of my current company has big differences from my previous one. This startup provides me with a platform to try something out of the box but also forces me to start from scratch.</p>"},{"location":"year_review_2022/#life-and-growth","title":"Life and growth","text":"<p>In 2022, despite COVID-19 and lockdowns, I travelled several times, most of which were side trips. For the new year, the softening of the COVID-19 control strategies indicates more opportunities to go overseas. So, I expect to travel overseas at least once this year!</p> <p>For personal development, in 2023, I\u2019ll make plans to equip myself with more tech skills. Of course, learning new skills takes courage, and I'm willing to be a beginner, to experience the awkwardness of not knowing how things work. We are wired to seek comfort and avoid failure. It may seem counterintuitive, but I believe that the faster you fail, adjust and try again, the faster you succeed.</p> <p>Here, I'll list my professional plans for 2023: - Set up my personal GitHub Pages - Write at lease one technical blog twice a week - Write for InfoQ and CNCF - Pass AWS Cloud Practitioner Certification by January - Grasp frontend programming languages, including HTML, CSS, and JavaScript, and get certified on freeCodeCamp by April - Get more familiar with Docker and Kubernetes, and get certified by KCNA by the end of July - Finish Foundations of Digital Marketing nad E-commerce on Coursera by September</p>"},{"location":"year_review_2022/#move-forward","title":"Move forward","text":"<p>Layoffs have been the buzzword and hashtag of the year 2022. As tech giants Meta, Twitter, among many other big and small fish in the business ecosystem, undertake a firing spree, they are leaving several employees emotionally and mentally wounded. For some, the financial anxiety of losing a job is the real pinch. But for most, the emotional upheaval that a layoff brings, is the deeper wound that takes time to heal.</p> <p>Working for a tech company without IT background, I also suffered from insecurity and sometimes even undervalued myself. This sense became more intense when my company started to make redundancies and my peers doing similar things got laid off all of a sudden. However, it may be normal to feel helpless, hapless and hopeless, all at the same time. But we have got to find inner strength to rebuild our confidence after a layoff, and bounce back.</p> <p>I\u2019m hopeful for 2023 despite many clouds and uncertainties on the horizon. I hope to meet and enjoy time with my friends and dearest ones in the coming year.</p> <p>Happy 2023!</p>"},{"location":"intermediate-ml/categorical-variables/","title":"Categorical Variables","text":"<p>In this tutorial, you will learn what a categorical variable is, along with three approaches for handling this type of data.</p>"},{"location":"intermediate-ml/categorical-variables/#introduction","title":"Introduction","text":"<p>A categorical variable takes only a limited number of values.</p> <ul> <li> <p>Consider a survey that asks how often you eat breakfast and provides four options: \"Never\", \"Rarely\", \"Most days\", or \"Every day\". In this case, the data is categorical, because responses fall into a fixed set of categories.</p> </li> <li> <p>If people responded to a survey about which brand of car they owned, the responses would fall into categories like \"Honda\", \"Toyota\", and \"Ford\". In this case, the data is also categorical.</p> </li> </ul> <p>You will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first. In this tutorial, we'll compare three approaches that you can use to prepare your categorical data.</p>"},{"location":"intermediate-ml/categorical-variables/#three-approaches","title":"Three Approaches","text":"<ol> <li>Drop Categorical Variables</li> </ol> <p>The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.</p> <ol> <li>Ordinal Encoding</li> </ol> <p>Ordinal encoding assigns each unique value to a different integer.</p> <p></p> <p>This approach assumes an ordering of the categories: \"Never\" (0) &lt; \"Rarely\" (1) &lt; \"Most days\" (2) &lt; \"Every day\" (3).</p> <p>This assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree-based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables.</p> <ol> <li>One-hot Encoding</li> </ol> <p>One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data. To understand this, we'll work through an example.</p> <p></p> <p>In the original dataset, \"Color\" is a categorical variable with three categories: \"Red\", \"Yellow\", and \"Green\". The corresponding one-hot encoding contains one column for each possible value, and one row for each row in the original dataset. Wherever the original value was \"Red\", we put a 1 in the \"Red\" column; if the original value was \"Yellow\", we put a 1 in the \"Yellow\" column, and so on.</p> <p>In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., \"Red\" is neither more nor less than \"Yellow\"). We refer to categorical variables without an intrinsic ranking as nominal variables.</p> <p>One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values).</p>"},{"location":"intermediate-ml/categorical-variables/#example","title":"Example","text":"<p>As in the previous tutorial, we will work with the Melbourne Housing dataset.</p> <p>We won't focus on the data loading step. Instead, you can imagine you are at a point where you already have the training and validation data in <code>X_train</code>, <code>X_valid</code>, <code>y_train</code>, and <code>y_valid</code>.</p> Python<pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n# Read the data\ndata = pd.read_csv('../dataset/melb_data.csv')\n# Separate target from predictors\ny = data.Price\nX = data.drop(['Price'], axis=1)\n# Divide data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\nrandom_state=0)\n# Drop columns with missing values (simplest approach)\ncols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \nX_train_full.drop(cols_with_missing, axis=1, inplace=True)\nX_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() &lt; 10 and \nX_train_full[cname].dtype == \"object\"]\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n</code></pre> <p>We take a peek at the training data with the head() method below.</p> Python<pre><code>X_train.head()\n</code></pre> <p>Next, we obtain a list of all of the categorical variables in the training data.</p> <p>We do this by checking the data type (or dtype) of each column. The <code>object</code> dtype indicates a column has text (there are other things it could theoretically be, but that's unimportant for our purposes). For this dataset, the columns with text indicate categorical variables.</p> Python<pre><code># Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\nprint(\"Categorical variables:\")\nprint(object_cols)\n</code></pre>"},{"location":"intermediate-ml/categorical-variables/#define-function-to-measure-quality-of-each-approach","title":"Define Function to Measure Quality of Each Approach","text":"<p>We define a function <code>score_dataset()</code> to compare the three different approaches to dealing with categorical variables. This function reports the mean absolute error (MAE) from a random forest model. In general, we want the MAE to be as low as possible!</p> Python<pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_valid)\nreturn mean_absolute_error(y_valid, preds)\n</code></pre>"},{"location":"intermediate-ml/categorical-variables/#score-from-approach-1-drop-categorical-variables","title":"Score from Approach 1 (Drop Categorical Variables)","text":"<p>We drop the <code>object</code> columns with the <code>select_dtypes()</code> method.</p> Python<pre><code>drop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\nprint(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n</code></pre>"},{"location":"intermediate-ml/categorical-variables/#score-from-approach-2-ordinal-encoding","title":"Score from Approach 2 (Ordinal Encoding)","text":"<p>Scikit-learn has a <code>OrdinalEncoder</code> class that can be used to get ordinal encodings. We loop over the categorical variables and apply the ordinal encoder separately to each column.</p> Python<pre><code>from sklearn.preprocessing import OrdinalEncoder\n# Make copy to avoid changing original data \nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n# Apply ordinal encoder to each column with categorical data\nordinal_encoder = OrdinalEncoder()\nlabel_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\nlabel_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\nprint(\"MAE from Approach 2 (Ordinal Encoding):\") \nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n</code></pre> <p>In the code cell above, for each column, we randomly assign each unique value to a different integer. This is a common approach that is simpler than providing custom labels; however, we can expect an additional boost in performance if we provide better-informed labels for all ordinal variables.</p>"},{"location":"intermediate-ml/categorical-variables/#score-from-approach-3-one-hot-encoding","title":"Score from Approach 3 (One-Hot Encoding)","text":"<p>We use the <code>OneHotEncoder</code> class from <code>scikit-learn</code> to get one-hot encodings. There are a number of parameters that can be used to customize its behavior.</p> <ul> <li>We set <code>handle_unknown='ignore'</code> to avoid errors when the validation data contains classes that aren't represented in the training data, and</li> <li>setting sparse=False ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).</li> </ul> <p>To use the encoder, we supply only the categorical columns that we want to be one-hot encoded. For instance, to encode the training data, we supply <code>X_train[object_cols]</code>. (<code>object_cols</code> in the code cell below is a list of the column names with categorical data, and so <code>X_train[object_cols]</code> contains all of the categorical data in the training set.)</p> Python<pre><code>from sklearn.preprocessing import OneHotEncoder\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n# Ensure all columns have string type\nOH_X_train.columns = OH_X_train.columns.astype(str)\nOH_X_valid.columns = OH_X_valid.columns.astype(str)\nprint(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))\n</code></pre>"},{"location":"intermediate-ml/categorical-variables/#which-approach-is-best","title":"Which approach is best?","text":"<p>In this case, dropping the categorical columns (Approach 1) performed worst, since it had the highest MAE score. As for the other two approaches, since the returned MAE scores are so close in value, there doesn't appear to be any meaningful benefit to one over the other.</p> <p>In general, one-hot encoding (Approach 3) will typically perform best, and dropping the categorical columns (Approach 1) typically performs worst, but it varies on a case-by-case basis.</p>"},{"location":"intermediate-ml/categorical-variables/#conclusion","title":"Conclusion","text":"<p>The world is filled with categorical data. You will be a much more effective data scientist if you know how to use this common data type!</p>"},{"location":"intermediate-ml/data-leakage/","title":"Data Leakage","text":"<p>Data leakage happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production.</p> <p>In other words, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate.</p> <p>There are two main types of leakage: target leakage and train-test contamination.</p>"},{"location":"intermediate-ml/data-leakage/#target-leakage","title":"Target Leakage","text":"<p>Target leakage occurs when your predictors include data that will not be available at the time you make predictions. It is important to think about target leakage in terms of the timing or chronological order that data becomes available, not merely whether a feature helps make good predictions.</p> <p>An example will be helpful. Imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data look like this:</p> got_pneumonia age weight male took_antibiotic_medicine False 65 100 False False False 72 130 True False True 58 100 False True <p>Note</p> <p>People take antibiotic medicines after getting pneumonia in order to recover. The raw data shows a strong relationship between those columns, but <code>took_antibiotic_medicine</code> is frequently changed after the value for <code>got_pneumonia</code> is determined. This is target leakage.</p> <p>The model would see that anyone who has a value of <code>False</code> for <code>took_antibiotic_medicine</code> didn't have pneumonia. Since validation data comes from the same source as training data, the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores.</p> <p>But the model will be very inaccurate when subsequently deployed in the real world, because even patients who will get pneumonia won't have received antibiotics yet when we need to make predictions about their future health.</p> <p>To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded.</p>"},{"location":"intermediate-ml/data-leakage/#train-test-contamination","title":"Train-test Contamination","text":"<p>A different type of leak occurs when you aren't careful to distinguish training data from validation data.</p> <p>Recall that validation is meant to be a measure of how the model does on data that it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavior. This is sometimes called train-test contamination.</p> <p>For example, imagine you run preprocessing (like fitting an imputer for missing values) before calling <code>train_test_split()</code>. The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.</p> <p>After all, you incorporated data from the validation or test data into how you make predictions, so it may do well on that particular data even if it can't generalize to new data. This problem becomes even more subtle (and more dangerous) when you do more complex feature engineering.</p> <p>If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit-learn pipelines. When using cross-validation, it's even more critical that you do your preprocessing inside the pipeline!</p>"},{"location":"intermediate-ml/data-leakage/#example","title":"Example","text":"<p>In this example, you will learn one way to detect and remove target leakage.</p> <p>We will use a dataset about credit card applications and skip the basic data set-up code. The end result is that information about each credit card application is stored in a DataFrame <code>X</code>. We'll use it to predict which applications were accepted in a Series <code>y</code>.</p> Python<pre><code>import pandas as pd\n# Read the data\ndata = pd.read_csv('../input/aer-credit-card-data/AER_credit_card_data.csv', \ntrue_values = ['yes'], false_values = ['no'])\n# Select target\ny = data.card\n# Select predictors\nX = data.drop(['card'], axis=1)\nprint(\"Number of rows in the dataset:\", X.shape[0])\nX.head()\n</code></pre> <p>Since this is a small dataset, we will use cross-validation to ensure accurate measures of model quality.</p> Python<pre><code>from sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n# Since there is no preprocessing, we don't need a pipeline (used anyway as best practice!)\nmy_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))\ncv_scores = cross_val_score(my_pipeline, X, y, \ncv=5,\nscoring='accuracy')\nprint(\"Cross-validation accuracy: %f\" % cv_scores.mean())\n</code></pre> <p>With experience, you'll find that it's very rare to find models that are accurate 98% of the time. It happens, but it's uncommon enough that we should inspect the data more closely for target leakage.</p> <p>Here is a summary of the data, which you can also find under the data tab:</p> <ul> <li><code>card</code>: 1 if credit card application accepted, 0 if not</li> <li><code>reports</code>: Number of major derogatory reports</li> <li><code>age</code>: Age n years plus twelfths of a year</li> <li><code>income</code>: Yearly income (divided by 10,000)</li> <li><code>share</code>: Ratio of monthly credit card expenditure to yearly income</li> <li><code>expenditure</code>: Average monthly credit card expenditure</li> <li><code>owner</code>: 1 if owns home, 0 if rents</li> <li><code>selfempl</code>: 1 if self-employed, 0 if not</li> <li><code>dependents</code>: 1 + number of dependents</li> <li><code>months</code>: Months living at current address</li> <li><code>majorcards</code>: Number of major credit cards held</li> <li><code>active</code>: Number of active credit accounts</li> </ul> <p>A few variables look suspicious. For example, does <code>expenditure</code> mean expenditure on this card or on cards used before applying?</p> <p>At this point, basic data comparisons can be very helpful:</p> Python<pre><code>expenditures_cardholders = X.expenditure[y]\nexpenditures_noncardholders = X.expenditure[~y]\nprint('Fraction of those who did not receive a card and had no expenditures: %.2f' \\\n      %((expenditures_noncardholders == 0).mean()))\nprint('Fraction of those who received a card and had no expenditures: %.2f' \\\n      %(( expenditures_cardholders == 0).mean()))\n# output:\n# Fraction of those who did not receive a card and had no expenditures: 1.00\n# Fraction of those who received a card and had no expenditures: 0.02\n</code></pre> <p>As shown above, everyone who did not receive a card had no expenditures, while only 2% of those who received a card had no expenditures. It's not surprising that our model appeared to have a high accuracy. But this also seems to be a case of target leakage, where expenditures probably means expenditures on the card they applied for.</p> <p>Since <code>share</code> is partially determined by <code>expenditure</code>, it should be excluded too. The variables <code>active</code> and <code>majorcards</code> are a little less clear, but from the description, they sound concerning. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.</p> <p>We would run a model without target leakage as follows:</p> Python<pre><code># Drop leaky predictors from dataset\npotential_leaks = ['expenditure', 'share', 'active', 'majorcards']\nX2 = X.drop(potential_leaks, axis=1)\n# Evaluate the model with leaky predictors removed\ncv_scores = cross_val_score(my_pipeline, X2, y, \ncv=5,\nscoring='accuracy')\nprint(\"Cross-val accuracy: %f\" % cv_scores.mean())\n</code></pre> <p>This accuracy is quite a bit lower, which might be disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would likely do much worse than that (in spite of its higher apparent score in cross-validation).</p>"},{"location":"intermediate-ml/data-leakage/#conclusion","title":"Conclusion","text":"<p>Data leakage can be multi-million dollar mistake in many data science applications. Careful separation of training and validation data can prevent train-test contamination, and pipelines can help implement this separation. Likewise, a combination of caution, common sense, and data exploration can help identify target leakage.</p>"},{"location":"intermediate-ml/missing-values/","title":"Missing Values","text":"<p>In this tutorial, you will learn three approaches to dealing with missing values. Then you'll compare the effectiveness of these approaches on a real-world dataset.</p>"},{"location":"intermediate-ml/missing-values/#introduction","title":"Introduction","text":"<p>There are many ways data can end up with missing values. For example,</p> <ul> <li>A 2 bedroom house won't include a value for the size of a third bedroom.</li> <li>A survey respondent may choose not to share his income.</li> </ul> <p>Most machine learning libraries (including scikit-learn) give an error if you try to build a model using data with missing values. So you'll need to choose one of the strategies below.</p>"},{"location":"intermediate-ml/missing-values/#three-approaches","title":"Three Approaches","text":""},{"location":"intermediate-ml/missing-values/#a-simple-option-drop-columns-with-missing-values","title":"A Simple Option: Drop Columns with Missing Values","text":"<p>The simplest option is to drop columns with missing values.</p> <p></p> <p>Unless most values in the dropped columns are missing, the model loses access to a lot of (potentially useful!) information with this approach. As an extreme example, consider a dataset with 10,000 rows, where one important column is missing a single entry. This approach would drop the column entirely!</p>"},{"location":"intermediate-ml/missing-values/#a-better-option-imputation","title":"A Better Option: Imputation","text":"<p>Imputation fills in the missing values with some number. For instance, we can fill in the mean value along each column.</p> <p></p> <p>The imputed value won't be exactly right in most cases, but it usually leads to more accurate models than you would get from dropping the column entirely.</p>"},{"location":"intermediate-ml/missing-values/#an-extension-to-imputation","title":"An Extension To Imputation","text":"<p>Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing.</p> <p></p> <p>In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries.</p> <p>In some cases, this will meaningfully improve results. In other cases, it doesn't help at all.</p>"},{"location":"intermediate-ml/missing-values/#example","title":"Example","text":"<p>In the example, we will work with the Melbourne Housing dataset. Our model will use information such as the number of rooms and land size to predict home price.</p> <p>We won't focus on the data loading step. Instead, you can imagine you are at a point where you already have the training and validation data in <code>X_train</code>, <code>X_valid</code>, <code>y_train</code>, and <code>y_valid</code>.</p> Python<pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n# Load the data\ndata = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n# Select target\ny = data.Price\n# To keep things simple, we'll use only numerical predictors\nmelb_predictors = data.drop(['Price'], axis=1)\nX = melb_predictors.select_dtypes(exclude=['object'])\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\nrandom_state=0)\n</code></pre>"},{"location":"intermediate-ml/missing-values/#define-function-to-measure-quality-of-each-approach","title":"Define Function to Measure Quality of Each Approach","text":"<p>We define a function <code>score_dataset()</code> to compare different approaches to dealing with missing values. This function reports the mean absolute error (MAE) from a random forest model.</p> Python<pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\nmodel = RandomForestRegressor(n_estimators=10, random_state=0)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_valid)\nreturn mean_absolute_error(y_valid, preds)\n</code></pre>"},{"location":"intermediate-ml/missing-values/#score-from-approach-1-drop-columns-with-missing-values","title":"Score from Approach 1 (Drop Columns with Missing Values)","text":"<p>Since we are working with both training and validation sets, we are careful to drop the same columns in both DataFrames.</p> Python<pre><code># Get names of columns with missing values\ncols_with_missing = [col for col in X_train.columns\nif X_train[col].isnull().any()]\n# Drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\nprint(\"MAE from Approach 1 (Drop columns with missing values):\")\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n</code></pre>"},{"location":"intermediate-ml/missing-values/#score-from-approach-2-imputation","title":"Score from Approach 2 (Imputation)","text":"<p>Next, we use <code>SimpleImputer</code> to replace missing values with the mean value along each column.</p> <p>Although it's simple, filling in the mean value generally performs quite well (but this varies by dataset). While statisticians have experimented with more complex ways to determine imputed values (such as regression imputation, for instance), the complex strategies typically give no additional benefit once you plug the results into sophisticated machine learning models.</p> Python<pre><code>from sklearn.impute import SimpleImputer\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\nprint(\"MAE from Approach 2 (Imputation):\")\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\n</code></pre> <p>We see that Approach 2 has lower MAE than Approach 1, so Approach 2 performed better on this dataset.</p>"},{"location":"intermediate-ml/missing-values/#score-from-approach-3-an-extension-to-imputation","title":"Score from Approach 3 (An Extension to Imputation)","text":"<p>Next, we impute the missing values, while also keeping track of which values were imputed.</p> Python<pre><code># Make copy to avoid changing original data (when imputing)\nX_train_plus = X_train.copy()\nX_valid_plus = X_valid.copy()\n# Make new columns indicating what will be imputed\nfor col in cols_with_missing:\nX_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\nX_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\nimputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n# Imputation removed column names; put them back\nimputed_X_train_plus.columns = X_train_plus.columns\nimputed_X_valid_plus.columns = X_valid_plus.columns\nprint(\"MAE from Approach 3 (An Extension to Imputation):\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))\n</code></pre> <p>As we can see, Approach 3 performed slightly worse than Approach 2.</p> <p>So, why did imputation perform better than dropping the columns?</p> <p>The training data has 10864 rows and 12 columns, where three columns contain missing data. For each column, less than half of the entries are missing. Thus, dropping the columns removes a lot of useful information, and so it makes sense that imputation would perform better.</p> Python<pre><code># Shape of training data (num_rows, num_columns)\nprint(X_train.shape)\n# Number of missing values in each column of training data\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column &gt; 0])\n</code></pre>"},{"location":"intermediate-ml/missing-values/#conclusion","title":"Conclusion","text":"<p>As is common, imputing missing values (in Approach 2 and Approach 3) yielded better results, relative to when we simply dropped columns with missing values (in Approach 1).</p>"},{"location":"intermediate-ml/missing-values/#your-turn","title":"Your Turn","text":"<p>Compare these approaches to dealing with missing values yourself in this exercise!</p>"},{"location":"intermediate-ml/pipelines/","title":"Pipelines","text":"<p>In this tutorial, you will learn how to use pipelines to clean up your modeling code.</p>"},{"location":"intermediate-ml/pipelines/#introduction","title":"Introduction","text":"<p>Pipelines are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.</p> <p>Many data scientists hack together models without pipelines, but pipelines have some important benefits. Those include:</p> <ul> <li>Cleaner Code: Accounting for data at each step of preprocessing can get messy. With a pipeline, you won't need to manually keep track of your training and validation data at each step.</li> <li>Fewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step.</li> <li>Easier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.</li> <li>More Options for Model Validation: You will see an example in the next tutorial, which covers cross-validation.</li> </ul>"},{"location":"intermediate-ml/pipelines/#example","title":"Example","text":"<p>As in the previous tutorial, we will work with the Melbourne Housing dataset.</p> <p>We won't focus on the data loading step. Instead, you can imagine you are at a point where you already have the training and validation data in <code>X_train</code>, <code>X_valid</code>, <code>y_train</code>, and <code>y_valid</code>.</p> <p>We construct the full pipeline in three steps.</p>"},{"location":"intermediate-ml/pipelines/#step-1-define-preprocessing-steps","title":"Step 1: Define Preprocessing Steps","text":"<p>Similar to how a pipeline bundles together preprocessing and modeling steps, we use the <code>ColumnTransformer</code> class to bundle together different preprocessing steps. The code below:</p> <ul> <li>imputes missing values in numerical data, and</li> <li>imputes missing values and applies a one-hot encoding to categorical data.</li> </ul> Python<pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n('imputer', SimpleImputer(strategy='most_frequent')),\n('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\ntransformers=[\n('num', numerical_transformer, numerical_cols),\n('cat', categorical_transformer, categorical_cols)\n])\n</code></pre>"},{"location":"intermediate-ml/pipelines/#step-2-define-the-model","title":"Step 2: Define the Model","text":"<p>Next, we define a random forest model with the familiar <code>RandomForestRegressor</code> class.</p> Python<pre><code>from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n</code></pre>"},{"location":"intermediate-ml/pipelines/#step-3-create-and-evaluate-the-pipeline","title":"Step 3: Create and Evaluate the Pipeline","text":"<p>Finally, we use the <code>Pipeline</code> class to define a pipeline that bundles the preprocessing and modeling steps. There are a few important things to notice:</p> <ul> <li>With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!)</li> <li>With the pipeline, we supply the unprocessed features in <code>X_valid</code> to the <code>predict()</code> command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.)</li> </ul> Python<pre><code>from sklearn.metrics import mean_absolute_error\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n('model', model)\n])\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)\n</code></pre>"},{"location":"intermediate-ml/pipelines/#conclusion","title":"Conclusion","text":"<p>Pipelines are valuable for cleaning up machine learning code and avoiding errors, and are especially useful for workflows with sophisticated data preprocessing.</p>"},{"location":"intermediate-ml/xgboost/","title":"XGBoost","text":"<p>In this tutorial, you will learn how to build and optimize models with gradient boosting. This method dominates many Kaggle competitions and achieves state-of-the-art results on a variety of datasets.</p>"},{"location":"intermediate-ml/xgboost/#introduction","title":"Introduction","text":"<p>For much of this course, you have made predictions with the random forest method, which achieves better performance than a single decision tree simply by averaging the predictions of many decision trees.</p> <p>We refer to the random forest method as an \"ensemble method\". By definition, ensemble methods combine the predictions of several models (e.g., several trees, in the case of random forests).</p> <p>Next, we'll learn about another ensemble method called gradient boosting.</p>"},{"location":"intermediate-ml/xgboost/#gradient-boosting","title":"Gradient Boosting","text":"<p>Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble.</p> <p>It begins by initializing the ensemble with a single model, whose predictions can be pretty naive. Even if its predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.</p> <p>Then, we start the cycle:</p> <ul> <li>First, we use the current ensemble to generate predictions for each observation in the dataset. To make a prediction, we add the predictions from all models in the ensemble.</li> <li>These predictions are used to calculate a loss function, like mean squared error, for instance.</li> <li>Then, we use the loss function to fit a new model that will be added to the ensemble. Specifically, we determine model parameters so that adding this new model to the ensemble will reduce the loss.</li> <li>Finally, we add the new model to the ensemble, and ...</li> <li>...repeat!</li> </ul> <p></p>"},{"location":"intermediate-ml/xgboost/#example","title":"Example","text":"<p>We begin by loading the training and validation data in <code>X_train</code>, <code>X_valid</code>, <code>y_train</code>, and <code>y_valid</code>.</p> Python<pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n# read data\ndata = pd.read_csv(\"../dataset/melb_data.csv\")\n# select subset of predictors\ncols_to_use = ['Rooms','Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\n# select target\ny = data.Price\n# separate data into training and validation data\nX_train, X_valid, y_train, y_valid = train_test_split(X,y)\n</code></pre> <p>In this example, you'll work with the XGBoost library. XGBoost stands for extreme gradient boosting, which is an implementation of gradient boosting with several additional features focused on performance and speed. (Scikit-learn has another version of gradient boosting, but XGBoost has some technical advantages.)</p> <p>In the next code cell, we import the scikit-learn API for XGBoost (<code>xgboost.XGBRegressor</code>). This allows us to build and fit a model just as we would in scikit-learn. As you'll see in the output, the <code>XGBRegressor</code> class has many tunable parameters -- you'll learn about those soon!</p> Python<pre><code>from xgboost import XGBRegressor\nmy_model = XGBRegressor()\nmy_model.fit(X_train,y_train)\n</code></pre> <p>We also make predictions and evaluate the model.</p> Python<pre><code>from sklearn.metrics import mean_absolute_error\npredictions = my_model.predict(X_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))\n</code></pre>"},{"location":"intermediate-ml/xgboost/#parameter-tuning","title":"Parameter Tuning","text":"<p>XGBoost has a few parameters that can dramatically affect accuracy and training speed. The first parameters you should understand are:</p> <ul> <li><code>n_estimators</code> specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble.</li> <li>Too low a value causes underfitting, which leads to inaccurate predictions on both training data and test data.</li> <li>Too high a value causes overfitting, which causes accurate predictions on training data, but inaccurate predictions on test data (which is what we care about).</li> </ul> <p>Typical values range from 100-1000, though this depends a lot on the <code>learning_rate</code> parameter discussed below.</p> <p>Here is the code to set the number of models in the ensemble:</p> Python<pre><code>my_model = XGBRegressor(n_estimators=500)\nmy_model.fit(X_train, y_train)\n</code></pre> <p><code>early_stopping_rounds</code> offers a way to automatically find the ideal value for <code>n_estimators</code>. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for <code>n_estimators</code>. It's smart to set a high value for <code>n_estimators</code> and then use <code>early_stopping_rounds</code> to find the optimal time to stop iterating.</p> <p>Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. Setting <code>early_stopping_rounds=5</code> is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation scores.</p> <p>When using <code>early_stopping_rounds</code>, you also need to set aside some data for calculating the validation scores - this is done by setting the <code>eval_set</code> parameter.</p> <p>We can modify the example above to include early stopping:</p> Python<pre><code>my_model = XGBRegressor(n_estimators=500)\nmy_model.fit(X_train, y_train, \nearly_stopping_rounds=5, \neval_set=[(X_valid, y_valid)],\nverbose=False)\n</code></pre> <p>If you later want to fit a model with all of your data, set <code>n_estimators</code> to whatever value you found to be optimal when run with early stopping.</p> <p><code>learning_rate</code></p> <p>Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the learning rate) before adding them in.</p> <p>This means each tree we add to the ensemble helps us less. So, we can set a higher value for <code>n_estimators</code> without overfitting. If we use early stopping, the appropriate number of trees will be determined automatically.</p> <p>In general, a small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets <code>learning_rate=0.1</code>.</p> <p>Modifying the example above to change the learning rate yields the following code:</p> Python<pre><code>my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(X_train, y_train, \nearly_stopping_rounds=5, \neval_set=[(X_valid, y_valid)], \nverbose=False)\n</code></pre> <p><code>n_jobs</code></p> <p>On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter <code>n_jobs</code> equal to the number of cores on your machine. On smaller datasets, this won't help.</p> <p>The resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command.</p> <p>Here's the modified example:</p> Python<pre><code>my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_model.fit(X_train, y_train, \nearly_stopping_rounds=5, \neval_set=[(X_valid, y_valid)], \nverbose=False)\n</code></pre>"},{"location":"intermediate-ml/xgboost/#conclusion","title":"Conclusion","text":"<p>XGBoost is a leading software library for working with standard tabular data (the type of data you store in Pandas DataFrames, as opposed to more exotic types of data like images and videos). With careful parameter tuning, you can train highly accurate models.</p>"},{"location":"introduction-to-ml/basic-data-exploration/","title":"Basic Data Exploration","text":""},{"location":"introduction-to-ml/basic-data-exploration/#use-pandas-to-get-familiar-with-your-data","title":"Use Pandas to get familiar with your data","text":"<p>The first step in any machine learning project is familiarize yourself with the data. You'll use the Pandas library for this. Pandas is the primary tool data scientists use for exploring and manipulating data. Most people abbreviate pandas in their code as pd. We do this with the command:</p> Python<pre><code>import pandas as pd\n</code></pre> <p>The most important part of the Pandas library is the DataFrame. A DataFrame holds the type of data you might think of as a table. This is similar to a sheet in Excel, or a table in a SQL database.</p> <p>Pandas has powerful methods for most things you'll want to do with this type of data.</p> <p>As an example, we'll look at data about home prices in Melbourne, Australia. In the hands-on exercises, you will apply the same processes to a new dataset, which has home prices in Iowa.</p> <p>The example (Melbourne) data is at the file path ../input/melbourne-housing-snapshot/melb_data.csv.</p> <p>We load and explore the data with the following commands:</p> Python<pre><code># save filepath to variable for easier access\nmelbourne_file_path = \"../input/melbourne-housing-snapshot/melb_data.csv\"\n# read data and store it in DataFrame named `melbourne_data`\nmelbourne_data = pd.read_csv(melbourne_file_path)\n# provide a summary of the data\nmelbourne_data.describe()\n</code></pre>"},{"location":"introduction-to-ml/basic-data-exploration/#interprete-data-expression","title":"Interprete data expression","text":"<p>The output of the preceding command can be similar to the following:</p> <p></p> <p>The results show 8 numbers for each column in your original dataset. The first number, the count, shows how many rows have non-missing values.</p> <p>Missing values arise for many reasons. For example, the size of the 2nd bedroom wouldn't be collected when surveying a 1 bedroom house. We'll come back to the topic of missing data.</p> <p>The second value is the mean, which is the average. Under that, std is the standard deviation, which measures how numerically spread out the values are.</p> <p>To interpret the min, 25%, 50%, 75% and max values, imagine sorting each column from lowest to highest value. The first (smallest) value is the min. If you go a quarter way through the list, you'll find a number that is bigger than 25% of the values and smaller than 75% of the values. That is the 25% value (pronounced \"25th percentile\"). The 50th and 75th percentiles are defined analogously, and the max is the largest number.</p>"},{"location":"introduction-to-ml/basic-data-exploration/#your-turn","title":"Your turn","text":"<p>Try it out yourself in the Model Building Exercise.</p>"},{"location":"introduction-to-ml/how-models-work/","title":"How Models Work","text":""},{"location":"introduction-to-ml/how-models-work/#introduction","title":"Introduction","text":"<p>We'll start with an overview of how machine learning models work and how they are used. This may feel basic if you've done statistical modeling or machine learning before. Don't worry, we will progress to building powerful models soon.</p> <p>You ask your cousin how he's predicted real estate values in the past, and he says it is just intuition. But more questioning reveals that he's identified price patterns from houses he has seen in the past, and he uses those patterns to make predictions for new houses he is considering.</p> <p>Machine learning works the same way. We'll start with a model called the Decision Tree. There are fancier models that give more accurate predictions. But decision trees are easy to understand, and they are the basic building block for some of the best models in data science.</p> <p>For simplicity, we'll start with the simplest possible decision tree.</p> <p></p> <p>It divides houses into only two categories. The predicted price for any house under consideration is the historical average price of houses in the same category.</p> <p>We use data to decide how to break the houses into two groups, and then again to determine the predicted price in each group. This step of capturing patterns from data is called fitting or training the model. The data used to fit the model is called the training data.</p> <p>The details of how the model is fit (e.g. how to split up the data) is complex enough that we will save it for later. After the model has been fit, you can apply it to new data to predict prices of additional homes.</p>"},{"location":"introduction-to-ml/how-models-work/#improving-the-decision-tree","title":"Improving the decision tree","text":"<p>Which of the following two decision trees is more likely to result from fitting the real estate training data?</p> <p></p> <p>The decision tree on the left (Decision Tree 1) probably makes more sense, because it captures the reality that houses with more bedrooms tend to sell at higher prices than houses with fewer bedrooms. The biggest shortcoming of this model is that it doesn't capture most factors affecting home price, like number of bathrooms, lot size, location, etc.</p> <p>You can capture more factors using a tree that has more \"splits.\" These are called \"deeper\" trees. A decision tree that also considers the total size of each house's lot might look like this:</p> <p></p> <p>You predict the price of any house by tracing through the decision tree, always picking the path corresponding to that house's characteristics. The predicted price for the house is at the bottom of the tree. The point at the bottom where we make a prediction is called a leaf.</p> <p>The splits and values at the leaves will be determined by the data, so it's time for you to check out the data you will be working with.</p>"},{"location":"introduction-to-ml/how-models-work/#continue","title":"Continue","text":"<p>Let's get more specific. It's time to Examine Your Data.</p>"},{"location":"introduction-to-ml/model-validation/","title":"Model Validation","text":"<p>You've built a model. But how good is it?</p> <p>In this lesson, you will learn to use model validation to measure the quality of your model. Measuring model quality is the key to iteratively improving your models.</p>"},{"location":"introduction-to-ml/model-validation/#what-is-model-validation","title":"What is model validation","text":"<p>You'll want to evaluate almost every model you ever build. In most (though not all) applications, the relevant measure of model quality is predictive accuracy. In other words, will the model's predictions be close to what actually happens.</p> <p>Many people make a huge mistake when measuring predictive accuracy. They make predictions with their training data and compare those predictions to the target values in the training data. You'll see the problem with this approach and how to solve it in a moment, but let's think about how we'd do this first.</p> <p>You'd first need to summarize the model quality into an understandable way. If you compare predicted and actual home values for 10,000 houses, you'll likely find mix of good and bad predictions. Looking through a list of 10,000 predicted and actual values would be pointless. We need to summarize this into a single metric.</p> <p>There are many metrics for summarizing model quality, but we'll start with one called Mean Absolute Error (also called MAE). Let's break down this metric starting with the last word, error.</p> <p>The prediction error for each house is:</p> Text Only<pre><code>error=actual\u2212predicted\n</code></pre> <p>So, if a house cost $150,000 and you predicted it would cost $100,000 the error is $50,000.</p> <p>With the MAE metric, we take the absolute value of each error. This converts each error to a positive number. We then take the average of those absolute errors. This is our measure of model quality. In plain English, it can be said as</p> Text Only<pre><code>On average, our predictions are off by about X.\n</code></pre> <p>To calculate MAE, we first need a model. That is built in a hidden cell below, which you can review by clicking the code button.</p> Python<pre><code># Data Loading Code Hidden Here\nimport pandas as pd\n# Load data\nmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \n# Filter rows with missing price values\nfiltered_melbourne_data = melbourne_data.dropna(axis=0)\n# Choose target and features\ny = filtered_melbourne_data.Price\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n'YearBuilt', 'Lattitude', 'Longtitude']\nX = filtered_melbourne_data[melbourne_features]\nfrom sklearn.tree import DecisionTreeRegressor\n# Define model\nmelbourne_model = DecisionTreeRegressor()\n# Fit model\nmelbourne_model.fit(X, y)\n</code></pre> <p>Once we have a model, here is how we calculate the mean absolute error:</p> Python<pre><code>from sklearn.metrics import mean_absolute_error\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n</code></pre>"},{"location":"introduction-to-ml/model-validation/#the-problem-with-in-sample-scores","title":"The problem with \"in-sample\" scores","text":"<p>The measure we just computed can be called an \"in-sample\" score. We used a single \"sample\" of houses for both building the model and evaluating it. Here's why this is bad.</p> <p>Imagine that, in the large real estate market, door color is unrelated to home price.</p> <p>However, in the sample of data you used to build the model, all homes with green doors were very expensive. The model's job is to find patterns that predict home prices, so it will see this pattern, and it will always predict high prices for homes with green doors.</p> <p>Since this pattern was derived from the training data, the model will appear accurate in the training data.</p> <p>But if this pattern doesn't hold when the model sees new data, the model would be very inaccurate when used in practice.</p> <p>Since models' practical value come from making predictions on new data, we measure performance on data that wasn't used to build the model. The most straightforward way to do this is to exclude some data from the model-building process, and then use those to test the model's accuracy on data it hasn't seen before. This data is called validation data.</p>"},{"location":"introduction-to-ml/model-validation/#coding","title":"Coding","text":"<p>The scikit-learn library has a function <code>train_test_split</code> to break up the data into two pieces. We'll use some of that data as training data to fit the model, and we'll use the other data as validation data to calculate <code>mean_absolute_error</code>.</p> <p>Here is the code:</p> Python<pre><code>from sklearn.model_selection import train_test_split\n# split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n# Define model\nmelbourne_model = DecisionTreeRegressor()\n# Fit model\nmelbourne_model.fit(train_X, train_y)\n# get predicted prices on validation data\nval_predictions = melbourne_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))\n</code></pre>"},{"location":"introduction-to-ml/model-validation/#wow","title":"Wow!","text":"<p>Your mean absolute error for the in-sample data was about 500 dollars. Out-of-sample it is more than 250,000 dollars.</p> <p>This is the difference between a model that is almost exactly right, and one that is unusable for most practical purposes. As a point of reference, the average home value in the validation data is 1.1 million dollars. So the error in new data is about a quarter of the average home value.</p> <p>There are many ways to improve this model, such as experimenting to find better features or different model types.</p>"},{"location":"introduction-to-ml/model-validation/#your-turn","title":"Your Turn","text":"<p>Before we look at improving this model, try Model Validation for yourself.</p>"},{"location":"introduction-to-ml/random-forest/","title":"Random Forest","text":""},{"location":"introduction-to-ml/random-forest/#introduction","title":"Introduction","text":"<p>Decision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.</p> <p>Even today's most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We'll look at the random forest as an example.</p> <p>The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters.</p>"},{"location":"introduction-to-ml/random-forest/#example","title":"Example","text":"<p>You've already seen the code to load the data a few times. At the end of data-loading, we have the following variables:</p> <ul> <li>train_X</li> <li>val_X</li> <li>train_y</li> <li>val_y</li> </ul> Python<pre><code>import pandas as pd\n# Load data\nmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \n# Filter rows with missing values\nmelbourne_data = melbourne_data.dropna(axis=0)\n# Choose target and features\ny = melbourne_data.Price\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n'YearBuilt', 'Lattitude', 'Longtitude']\nX = melbourne_data[melbourne_features]\nfrom sklearn.model_selection import train_test_split\n# split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n</code></pre> <p>We build a random forest model similarly to how we built a decision tree in scikit-learn - this time using the <code>RandomForestRegressor</code> class instead of <code>DecisionTreeRegressor</code>.</p> Python<pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(train_X, train_y)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))\n</code></pre>"},{"location":"introduction-to-ml/random-forest/#conclusion","title":"Conclusion","text":"<p>There is likely room for further improvement, but this is a big improvement over the best decision tree error of 250,000. There are parameters which allow you to change the performance of the Random Forest much as we changed the maximum depth of the single decision tree. But one of the best features of Random Forest models is that they generally work reasonably even without this tuning.</p>"},{"location":"introduction-to-ml/random-forest/#your-turn","title":"Your turn","text":"<p>Try Using a Random Forest model yourself and see how much it improves your model.</p>"},{"location":"introduction-to-ml/underfitting-and-overfitting/","title":"Underfitting and Overfitting","text":"<p>At the end of this step, you will understand the concepts of underfitting and overfitting, and you will be able to apply these ideas to make your models more accurate.</p> <p>Experimenting With Different Models Now that you have a reliable way to measure model accuracy, you can experiment with alternative models and see which gives the best predictions. But what alternatives do you have for models?</p> <p>You can see in scikit-learn's documentation that the decision tree model has many options (more than you'll want or need for a long time). The most important options determine the tree's depth. Recall from the first lesson in this course that a tree's depth is a measure of how many splits it makes before coming to a prediction. This is a relatively shallow tree.</p> <p></p> <p>In practice, it's not uncommon for a tree to have 10 splits between the top level (all houses) and a leaf. As the tree gets deeper, the dataset gets sliced up into leaves with fewer houses. If a tree only had 1 split, it divides the data into 2 groups. If each group is split again, we would get 4 groups of houses. Splitting each of those again would create 8 groups. If we keep doubling the number of groups by adding more splits at each level, we'll have 210 groups of houses by the time we get to the 10th level. That's 1024 leaves.</p> <p>When we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes' actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses).</p> <p>This is a phenomenon called overfitting, where a model matches the training data almost perfectly, but does poorly in validation and other new data. On the flip side, if we make our tree very shallow, it doesn't divide up the houses into very distinct groups.</p> <p>At an extreme, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data (and it will be bad in validation too for the same reason). When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called underfitting.</p> <p>Since we care about accuracy on new data, which we estimate from our validation data, we want to find the sweet spot between underfitting and overfitting. Visually, we want the low point of the (red) validation curve in the figure below.</p> <p></p>"},{"location":"introduction-to-ml/underfitting-and-overfitting/#example","title":"Example","text":"<p>There are a few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes. But the <code>max_leaf_nodes</code> argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.</p> <p>We can use a utility function to help compare MAE scores from different values for <code>max_leaf_nodes</code>:</p> Python<pre><code>from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\nmodel = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\nmodel.fit(train_X, train_y)\npreds_val = model.predict(val_X)\nmae = mean_absolute_error(val_y, preds_val)\nreturn(mae)\n</code></pre> <p>The data is loaded into <code>train_X</code>, <code>val_X</code>, <code>train_y</code> and <code>val_y</code> using the code you've already seen (and which you've already written).</p> Python<pre><code># Data Loading Code Runs At This Point\nimport pandas as pd\n# Load data\nmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \n# Filter rows with missing values\nfiltered_melbourne_data = melbourne_data.dropna(axis=0)\n# Choose target and features\ny = filtered_melbourne_data.Price\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n'YearBuilt', 'Lattitude', 'Longtitude']\nX = filtered_melbourne_data[melbourne_features]\nfrom sklearn.model_selection import train_test_split\n# split data into training and validation data, for both features and target\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n</code></pre> <p>We can use a for-loop to compare the accuracy of models built with different values for max_leaf_nodes.</p> Python<pre><code># compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\nmy_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\nprint(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n</code></pre> <p>Of the options listed, 500 is the optimal number of leaves.</p>"},{"location":"introduction-to-ml/underfitting-and-overfitting/#conclusion","title":"Conclusion","text":"<p>Here's the takeaway: Models can suffer from either:</p> <ul> <li>Overfitting: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or</li> <li>Underfitting: failing to capture relevant patterns, again leading to less accurate predictions.</li> </ul> <p>We use validation data, which isn't used in model training, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one.</p>"},{"location":"introduction-to-ml/underfitting-and-overfitting/#your-turn","title":"Your turn","text":"<p>Try optimizing the model you've previously built.</p>"},{"location":"introduction-to-ml/your-first-ml-model/","title":"Your First Machine Learning Model","text":""},{"location":"introduction-to-ml/your-first-ml-model/#select-data-for-modeling","title":"Select data for modeling","text":"<p>Your dataset had too many variables to wrap your head around, or even to print out nicely. How can you pare down this overwhelming amount of data to something you can understand?</p> <p>We'll start by picking a few variables using our intuition. Later courses will show you statistical techniques to automatically prioritize variables.</p> <p>To choose variables/columns, we'll need to see a list of all columns in the dataset. That is done with the columns property of the DataFrame (the bottom line of code below).</p> Python<pre><code>import pandas as pd\nmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \nmelbourne_data.columns\n# dropna drops missing values (think of na as \"not available\")\nmelbourne_data = melbourne_data.dropna(axis=0)\n</code></pre> <p>There are many ways to select a subset of your data, and we will focus on two approaches for now.</p> <ul> <li>Dot notation, which we use to select the \"prediction target\".</li> <li>Selecting with a column list, which we use to select the \"features\".</li> </ul>"},{"location":"introduction-to-ml/your-first-ml-model/#select-the-prediction-target","title":"Select the prediction target","text":"<p>You can pull out a variable with dot-notation. This single column is stored in a Series, which is broadly like a DataFrame with only a single column of data.</p> <p>We'll use the dot notation to select the column we want to predict, which is called the prediction target. By convention, the prediction target is called y. So the code we need to save the house prices in the Melbourne data is</p> Python<pre><code>y = melbourne_data.Price\n</code></pre>"},{"location":"introduction-to-ml/your-first-ml-model/#choose-features","title":"Choose features","text":"<p>The columns that are inputted into our model (and later used to make predictions) are called \"features.\" In our case, those would be the columns used to determine the home price. Sometimes, you will use all columns except the target as features. Other times you'll be better off with fewer features.</p> <p>For now, we'll build a model with only a few features. Later on you'll see how to iterate and compare models built with different features.</p> <p>We select multiple features by providing a list of column names inside brackets. Each item in that list should be a string (with quotes).</p> <p>Here is an example:</p> Python<pre><code>melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\n</code></pre> <p>By convention, this data is called X.</p> Python<pre><code>X = melbourne_data[melbourne_features]\n</code></pre> <p>Let's quickly review the data we'll be using to predict house prices using the describe method and the head method, which shows the top few rows.</p> Python<pre><code>X.describe()\nX.head()\n</code></pre> <p>Visually checking your data with these commands is an important part of a data scientist's job. You'll frequently find surprises in the dataset that deserve further inspection.</p>"},{"location":"introduction-to-ml/your-first-ml-model/#build-your-model","title":"Build your model","text":"<p>You will use the scikit-learn library to create your models. When coding, this library is written as sklearn, as you will see in the sample code. Scikit-learn is easily the most popular library for modeling the types of data typically stored in DataFrames.</p> <p>The steps to building and using a model are:</p> <ul> <li>Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.</li> <li>Fit: Capture patterns from provided data. This is the heart of modeling.</li> <li>Predict: Just what it sounds like</li> <li>Evaluate: Determine how accurate the model's predictions are.</li> </ul> <p>Here is an example of defining a decision tree model with scikit-learn and fitting it with the features and target variable.</p> Python<pre><code>from sklearn.tree import DecisionTreeRegressor\n# Define model. Specify a number for random_state to ensure same results each run\nmelbourne_model = DecisionTreeRegressor(random_state=1)\n# Fit model\nmelbourne_model.fit(X, y)\n</code></pre> <p>Many machine learning models allow some randomness in model training. Specifying a number for <code>random_state</code> ensures you get the same results in each run. This is considered a good practice. You use any number, and model quality won't depend meaningfully on exactly what value you choose.</p> <p>We now have a fitted model that we can use to make predictions.</p> <p>In practice, you'll want to make predictions for new houses coming on the market rather than the houses we already have prices for. But we'll make predictions for the first few rows of the training data to see how the predict function works.</p> Python<pre><code>print(\"Making predictions for the following 5 houses:\")\nprint(X.head())\nprint(\"The predictions are\")\nprint(melbourne_model.predict(X.head()))\n</code></pre>"},{"location":"introduction-to-ml/your-first-ml-model/#your-turn","title":"Your turn","text":"<p>Try it out yourself in the Model Building Exercise.</p>"}]}